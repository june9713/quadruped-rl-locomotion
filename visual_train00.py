import time
import threading
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv
import gymnasium as gym
from collections import deque
import os
import argparse
import subprocess
import sys
import copy
import imageio.v2 as imageio
from datetime import datetime
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
from go1_mujoco_env import Go1MujocoEnv
import pandas as pd
from visual_training_callback import VisualTrainingCallback, VideoRecordingCallback, EnhancedVisualCallback
import torch
import glob
try:
    from go1_standing_env import Go1StandingEnv, GradualStandingEnv
except ImportError:
    print("âš ï¸ go1_standing_env.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
    raise

# í•œê¸€ í°íŠ¸ ì„¤ì •
font_name = 'Malgun Gothic'
plt.rc('font', family=font_name)
plt.rcParams['axes.unicode_minus'] = False


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='ê°•í™”í•™ìŠµ ì‹œê°ì  í›ˆë ¨')
    
    parser.add_argument('--task', type=str, default='standing', 
                       help='í›ˆë ¨í•  íƒœìŠ¤í¬ (ê¸°ë³¸ê°’: standing)')
    parser.add_argument('--pretrained_model', type=str, default=None,
                       help='ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--visual_interval', type=float, default=1.0,
                       help='ì‹œê°í™” ì£¼ê¸° (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0)')
    parser.add_argument('--show_duration', type=int, default=15,
                       help='ì‹œë®¬ë ˆì´ì…˜ ë³´ì—¬ì£¼ëŠ” ì‹œê°„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 15)')
    parser.add_argument('--save_videos', action='store_true',
                       help='ë¹„ë””ì˜¤ ì €ì¥ ì—¬ë¶€')
    parser.add_argument('--total_timesteps', type=int, default=3_000_000,
                       help='ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 3,000,000)')
    parser.add_argument('--num_envs', type=int, default=16,
                       help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (ê¸°ë³¸ê°’: 16)')
    parser.add_argument('--video_interval', type=int, default=100_000,
                       help='ë¹„ë””ì˜¤ ë…¹í™” ê°„ê²© (timesteps, ê¸°ë³¸ê°’: 100,000)')
    parser.add_argument('--use_curriculum', action='store_true',
                       help='ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‚¬ìš©')
    parser.add_argument('--learning_rate', type=float, default=3e-4,
                       help='í•™ìŠµë¥  (ê¸°ë³¸ê°’: 3e-4)')
    parser.add_argument('--batch_size', type=int, default=256,
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 256)')
    parser.add_argument('--n_steps', type=int, default=2048,
                       help='ë¡¤ì•„ì›ƒ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 2048)')
    parser.add_argument('--clip_range', type=float, default=0.2,
                       help='PPO í´ë¦½ ë²”ìœ„ (ê¸°ë³¸ê°’: 0.2)')
    parser.add_argument('--entropy_coef', type=float, default=0.01,
                       help='ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (ê¸°ë³¸ê°’: 0.01)')
    
    return parser.parse_args()


def create_ppo_model(env, args, tensorboard_log=None):
    """ê°œì„ ëœ PPO ëª¨ë¸ ìƒì„±"""
    
    # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµìš© í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
    if args.use_curriculum:
        def lr_schedule(progress_remaining):
            # ì´ˆë°˜ì—ëŠ” ë†’ì€ í•™ìŠµë¥ , í›„ë°˜ì—ëŠ” ë‚®ì€ í•™ìŠµë¥ 
            if progress_remaining > 0.8:
                return args.learning_rate
            elif progress_remaining > 0.5:
                return args.learning_rate * 0.5
            else:
                return args.learning_rate * 0.1
    else:
        lr_schedule = args.learning_rate
    
    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=lr_schedule,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=args.clip_range,
        clip_range_vf=None,
        ent_coef=args.entropy_coef,
        vf_coef=0.5,
        max_grad_norm=0.5,
        use_sde=False,  # ì´ˆë°˜ íƒìƒ‰ì„ ìœ„í•´ Trueë¡œ ì„¤ì • ê°€ëŠ¥
        sde_sample_freq=-1,
        target_kl=0.01,
        tensorboard_log=tensorboard_log,
        verbose=1,
        policy_kwargs=dict(
            net_arch=[dict(pi=[512, 512, 256], vf=[512, 512, 256])],
            activation_fn=torch.nn.ReLU,
            ortho_init=True,
        ),
        device='auto'
    )
    
    return model


def train_with_visual_feedback(args):
    """ê°œì„ ëœ ì‹œê°ì  í›ˆë ¨"""
    print(f"\nğŸš€ ê°œì„ ëœ ì‹œê°ì  í›ˆë ¨ ì‹œì‘! (task={args.task})")
    print(f"ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"  - í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"  - í´ë¦½ ë²”ìœ„: {args.clip_range}")
    print(f"  - ë³‘ë ¬ í™˜ê²½ ìˆ˜: {args.num_envs}")
    print(f"  - ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: {'ì‚¬ìš©' if args.use_curriculum else 'ë¯¸ì‚¬ìš©'}")
    
    # í™˜ê²½ ì„ íƒ
    if args.task == "standing":
        if args.use_curriculum:
            env_class = GradualStandingEnv
        else:
            env_class = Go1StandingEnv
    else:
        env_class = Go1MujocoEnv
    
    # í•™ìŠµìš© í™˜ê²½ (ë³‘ë ¬í™”)
    vec_env = make_vec_env(
        env_class, 
        n_envs=args.num_envs, 
        vec_env_cls=SubprocVecEnv,
        env_kwargs={'randomize_physics': True}  # Domain randomization í™œì„±í™”
    )
    
    # í‰ê°€ìš© í™˜ê²½
    eval_env = env_class(render_mode="rgb_array")
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EnhancedVisualCallback(  # ê°œì„ ëœ ì½œë°± ì‚¬ìš©
            eval_env,
            eval_interval_minutes=args.visual_interval,
            n_eval_episodes=5,
            show_duration_seconds=args.show_duration,
            save_videos=args.save_videos,
            use_curriculum=args.use_curriculum
        )
    ]
    
    # ë¹„ë””ì˜¤ ë…¹í™” ì½œë°±
    print("args.video_interval"  ,args.video_interval)
    if args.video_interval > 0:
        record_env = DummyVecEnv([lambda: env_class(render_mode="rgb_array")])
        callbacks.append(
            VideoRecordingCallback(
                record_env,
                record_interval_timesteps=args.video_interval,
                video_folder=f"eval_videos_{args.task}",
                show_duration_seconds=args.show_duration
            )
        )
    
    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    tensorboard_log = f"logs/{args.task}_enhanced_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if args.pretrained_model :
        modelPath = args.pretrained_model
        pretrained_model = modelPath
        if modelPath == "latest":
            models = glob.glob(f"./models/{args.task}*.zip")
            pretrained_model = list(sorted(models))[-1]
        elif os.path.exists(args.pretrained_model):
            #print(f"ğŸ“‚ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ: {args.pretrained_model}")
            pretrained_model = args.pretrained_model
        print(f"ğŸ“‚ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ: {pretrained_model}")
        model = PPO.load(pretrained_model, env=vec_env)
        model.set_env(vec_env)
    else:
        # torch import (PPO ë‚´ë¶€ì—ì„œ ì‚¬ìš©)
        try:
            import torch
        except ImportError:
            print("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install torch")
            return
        
        model = create_ppo_model(vec_env, args, tensorboard_log)
    
    # í•™ìŠµ ì‹œì‘
    try:
        print("\nğŸ¯ í•™ìŠµ ì‹œì‘...")
        print(f"ğŸ“Š TensorBoard ë¡œê·¸: {tensorboard_log}")
        print("ğŸ’¡ TensorBoard ì‹¤í–‰: tensorboard --logdir=logs\n")
        
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=False if args.pretrained_model else True
        )
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    # ìµœì¢… ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = f"training_reports_{args.task}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(report_path, exist_ok=True)
    
    if len(callbacks) > 0 and hasattr(callbacks[0], 'save_progress_report'):
        callbacks[0].save_progress_report(report_path)
        callbacks[0].save_detailed_analysis(report_path)  # ì¶”ê°€ ë¶„ì„
    
    # ëª¨ë¸ ì €ì¥
    model_path = f"models/{args.task}_enhanced_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    Path("models").mkdir(exist_ok=True)
    model.save(model_path)
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")
    
    # í›ˆë ¨ ì„¤ì • ì €ì¥
    config_path = os.path.join(report_path, "training_config.txt")
    with open(config_path, 'w') as f:
        f.write(f"Task: {args.task}\n")
        f.write(f"Total timesteps: {args.total_timesteps:,}\n")
        f.write(f"Num environments: {args.num_envs}\n")
        f.write(f"Learning rate: {args.learning_rate}\n")
        f.write(f"Batch size: {args.batch_size}\n")
        f.write(f"Clip range: {args.clip_range}\n")
        f.write(f"Entropy coefficient: {args.entropy_coef}\n")
        f.write(f"Curriculum learning: {args.use_curriculum}\n")
        f.write(f"Model saved at: {model_path}\n")
    
    # ì •ë¦¬
    eval_env.close()
    vec_env.close()
    if 'record_env' in locals():
        record_env.close()
    
    print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {report_path}")
    print(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. TensorBoard í™•ì¸: tensorboard --logdir=logs")
    print(f"   2. ë³´ê³ ì„œ í™•ì¸: {report_path}")
    print(f"   3. ëª¨ë¸ í…ŒìŠ¤íŠ¸: python test_model.py --model {model_path}")


if __name__ == "__main__":
    args = parse_arguments()
    train_with_visual_feedback(args)