import time
import threading
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv
import gymnasium as gym
from collections import deque
import os
import argparse
import subprocess
import sys
import copy
import imageio.v2 as imageio
from datetime import datetime
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
from go1_mujoco_env import Go1MujocoEnv
import pandas as pd
from visual_training_callback import VisualTrainingCallback, VideoRecordingCallback, EnhancedVisualCallback
import torch
import glob
try:
    from go1_standing_env import Go1StandingEnv, GradualStandingEnv, BipedalWalkingEnv
except ImportError:
    print("âš ï¸ go1_standing_env.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
    raise

# í•œê¸€ í°íŠ¸ ì„¤ì •
font_name = 'Malgun Gothic'
plt.rc('font', family=font_name)
plt.rcParams['axes.unicode_minus'] = False


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹± - 2ì¡± ë³´í–‰ ìµœì í™”"""
    parser = argparse.ArgumentParser(description='2ì¡± ë³´í–‰ ê°•í™”í•™ìŠµ ì‹œê°ì  í›ˆë ¨')
    
    parser.add_argument('--task', type=str, default='standing', 
                       help='í›ˆë ¨í•  íƒœìŠ¤í¬ (ê¸°ë³¸ê°’: standing)')
    parser.add_argument('--pretrained_model', type=str, default=None,
                       help='ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--visual_interval', type=float, default=1.0,
                       help='ì‹œê°í™” ì£¼ê¸° (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0)')
    parser.add_argument('--show_duration', type=int, default=15,
                       help='ì‹œë®¬ë ˆì´ì…˜ ë³´ì—¬ì£¼ëŠ” ì‹œê°„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 15)')
    parser.add_argument('--save_videos', action='store_true',
                       help='ë¹„ë””ì˜¤ ì €ì¥ ì—¬ë¶€')
    parser.add_argument('--total_timesteps', type=int, default=5_000_000,  # âœ… ì¦ê°€
                       help='ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 5,000,000)')
    parser.add_argument('--num_envs', type=int, default=12,  # âœ… ì•½ê°„ ì¶•ì†Œ (ì•ˆì •ì„±)
                       help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (ê¸°ë³¸ê°’: 12)')
    parser.add_argument('--video_interval', type=int, default=150_000,  # âœ… ë” ìì£¼
                       help='ë¹„ë””ì˜¤ ë…¹í™” ê°„ê²© (timesteps, ê¸°ë³¸ê°’: 150,000)')
    parser.add_argument('--use_curriculum', action='store_true',
                       help='ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‚¬ìš©')
    
    # âœ… 2ì¡± ë³´í–‰ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--learning_rate', type=float, default=2e-4,  # âœ… ì¶•ì†Œ (ì•ˆì •ì„±)
                       help='í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-4)')
    parser.add_argument('--batch_size', type=int, default=128,  # âœ… ì¶•ì†Œ (ì•ˆì •ì„±)
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 128)')
    parser.add_argument('--n_steps', type=int, default=1024,  # âœ… ì¶•ì†Œ (ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸)
                       help='ë¡¤ì•„ì›ƒ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 1024)')
    parser.add_argument('--clip_range', type=float, default=0.15,  # âœ… ì¶•ì†Œ (ì•ˆì •ì  ì—…ë°ì´íŠ¸)
                       help='PPO í´ë¦½ ë²”ìœ„ (ê¸°ë³¸ê°’: 0.15)')
    parser.add_argument('--entropy_coef', type=float, default=0.005,  # âœ… ì¶•ì†Œ (ëœ íƒí—˜ì )
                       help='ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (ê¸°ë³¸ê°’: 0.005)')
    
    # âœ… ìƒˆë¡œìš´ 2ì¡± ë³´í–‰ íŠ¹í™” íŒŒë¼ë¯¸í„°
    parser.add_argument('--target_vel', type=float, default=0.0,
                       help='ëª©í‘œ ì†ë„ (ê¸°ë³¸ê°’: 0.0 - ì œìë¦¬ ì„œê¸°)')
    parser.add_argument('--stability_weight', type=float, default=1.5,
                       help='ì•ˆì •ì„± ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.5)')
    parser.add_argument('--height_tolerance', type=float, default=0.12,
                       help='ë†’ì´ í—ˆìš© ì˜¤ì°¨ (ê¸°ë³¸ê°’: 0.12)')
    parser.add_argument('--early_stopping', action='store_true',
                       help='ì¡°ê¸° ì •ì§€ ì‚¬ìš© (ìˆ˜ë ´ ì‹œ)')
    parser.add_argument('--checkpoint_interval', type=int, default=500_000,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²© (ê¸°ë³¸ê°’: 500,000)')
    
    return parser.parse_args()


def create_optimized_ppo_model(env, args, tensorboard_log=None):
    """2ì¡± ë³´í–‰ ìµœì í™”ëœ PPO ëª¨ë¸ ìƒì„± - clip_range ì˜¤ë¥˜ ìˆ˜ì •"""
    
    # âœ… 2ì¡± ë³´í–‰ìš© í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
    def standing_lr_schedule(progress_remaining):
        """2ì¡± ë³´í–‰ ìµœì í™” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„"""
        if progress_remaining > 0.9:
            # ì´ˆê¸°: ë¹ ë¥¸ í•™ìŠµ
            return args.learning_rate * 1.2
        elif progress_remaining > 0.7:
            # ì¤‘ê¸°: ì•ˆì •ì  í•™ìŠµ
            return args.learning_rate
        elif progress_remaining > 0.3:
            # í›„ê¸°: ì„¸ë°€í•œ ì¡°ì •
            return args.learning_rate * 0.5
        else:
            # ë§ˆì§€ë§‰: ë§¤ìš° ì„¸ë°€í•œ ì¡°ì •
            return args.learning_rate * 0.2
    
    # âœ… ìˆ˜ì •: clip_rangeë„ í•¨ìˆ˜ë¡œ ì„¤ì •
    def clip_range_schedule(progress_remaining):
        """í´ë¦½ ë²”ìœ„ ìŠ¤ì¼€ì¤„"""
        return args.clip_range
    
    if args.use_curriculum:
        lr_schedule = standing_lr_schedule
        clip_range = clip_range_schedule
    else:
        lr_schedule = args.learning_rate
        clip_range = clip_range_schedule  # âœ… í•­ìƒ í•¨ìˆ˜ë¡œ ì„¤ì •
    
    # âœ… 2ì¡± ë³´í–‰ ìµœì í™” PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=lr_schedule,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=8,  # ì¶•ì†Œ (ê³¼ì í•© ë°©ì§€)
        gamma=0.995,  # ì¦ê°€ (ì¥ê¸° ì•ˆì •ì„± ì¤‘ì‹œ)
        gae_lambda=0.98,  # ì¦ê°€ (ì•ˆì •ì„±)
        clip_range=clip_range,  # âœ… í•¨ìˆ˜ë¡œ ì„¤ì •
        clip_range_vf=0.2,  # value function í´ë¦¬í•‘
        ent_coef=args.entropy_coef,
        vf_coef=0.8,  # ì¦ê°€ (value function ì¤‘ì‹œ)
        max_grad_norm=0.3,  # ì¶•ì†Œ (gradient ì•ˆì •ì„±)
        use_sde=False,
        sde_sample_freq=-1,
        target_kl=0.015,  # ì¦ê°€ (ì•ˆì •ì  ì—…ë°ì´íŠ¸)
        tensorboard_log=tensorboard_log,
        verbose=1,
        policy_kwargs=dict(
            net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])],  # ë„¤íŠ¸ì›Œí¬ ì¶•ì†Œ
            activation_fn=torch.nn.Tanh,  # Tanh ì‚¬ìš© (ì•ˆì •ì )
            ortho_init=True,
            log_std_init=-0.5,  # ì´ˆê¸° íƒí—˜ ì¶•ì†Œ
        ),
        device='auto'
    )
    
    return model


class StandingTrainingCallback(BaseCallback):
    """2ì¡± ë³´í–‰ íŠ¹í™” í›ˆë ¨ ì½œë°±"""
    
    def __init__(self, args, eval_env, verbose=0):
        super().__init__(verbose)
        self.args = args
        self.eval_env = eval_env
        self.best_reward = -np.inf
        self.no_improvement_steps = 0
        self.patience = 1_000_000
        
        # ì„±ëŠ¥ ì¶”ì 
        self.episode_rewards = deque(maxlen=100)
        self.success_rates = deque(maxlen=50)
        self.last_checkpoint = 0
        
        # âœ… ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ ì¶”ì 
        self.last_upside_down_count = 0
        
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ"""
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (self.num_timesteps - self.last_checkpoint >= self.args.checkpoint_interval):
            self._save_checkpoint()
            self.last_checkpoint = self.num_timesteps
            
        return True
    
    def _on_rollout_end(self) -> bool:
        """âœ… ë¡¤ì•„ì›ƒ ì¢…ë£Œ ì‹œ ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ë„ í•¨ê»˜ ì¶œë ¥"""
        
        # ê¸°ì¡´ ì„±ëŠ¥ í‰ê°€ ë¡œì§
        if len(self.locals.get('episode_rewards', [])) > 0:
            recent_rewards = self.locals['episode_rewards'][-10:]
            mean_reward = np.mean(recent_rewards)
            self.episode_rewards.extend(recent_rewards)
            
            # ê°œì„  ì¶”ì 
            if mean_reward > self.best_reward:
                self.best_reward = mean_reward
                self.no_improvement_steps = 0
                self._save_best_model()
            else:
                self.no_improvement_steps += self.args.n_steps * self.args.num_envs
        
        # âœ… ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ ìˆ˜ì§‘ ë° ì¶œë ¥
        self._log_upside_down_statistics()
        
        # ì¡°ê¸° ì •ì§€ í™•ì¸
        if (self.args.early_stopping and 
            self.no_improvement_steps > self.patience):
            print(f"\nğŸ›‘ ì¡°ê¸° ì •ì§€: {self.patience:,} ìŠ¤í… ë™ì•ˆ ê°œì„  ì—†ìŒ")
            return False
            
        return True


    def _log_upside_down_statistics(self):
        """âœ… ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ ë¡œê¹…"""
        try:
            # í™˜ê²½ì—ì„œ ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° ì¹´ìš´íŠ¸ ìˆ˜ì§‘
            upside_down_counts = []
            
            # ëª¨ë“  ë³‘ë ¬ í™˜ê²½ì—ì„œ í†µê³„ ìˆ˜ì§‘
            if hasattr(self.training_env, 'envs'):
                for env in self.training_env.envs:
                    try:
                        # í™˜ê²½ì˜ standing_rewardì—ì„œ ì¹´ìš´íŠ¸ ê°€ì ¸ì˜¤ê¸°
                        if hasattr(env, 'standing_reward'):
                            count = getattr(env.standing_reward, 'upside_down_count', 0)
                            upside_down_counts.append(count)
                        elif hasattr(env, 'env') and hasattr(env.env, 'standing_reward'):
                            # Wrapperê°€ ìˆëŠ” ê²½ìš°
                            count = getattr(env.env.standing_reward, 'upside_down_count', 0)
                            upside_down_counts.append(count)
                    except:
                        pass
            
            # í†µê³„ ê³„ì‚°
            if upside_down_counts:
                total_upside_down = sum(upside_down_counts)
                new_attempts = total_upside_down - self.last_upside_down_count
                avg_per_env = total_upside_down / len(upside_down_counts)
                
                # âœ… PPO ë¡œê·¸ì™€ í•¨ê»˜ ì¶œë ¥ë  ì¶”ê°€ ì •ë³´
                print(f"ğŸš¨ ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„:")
                print(f"   ì´ ì‹œë„ íšŸìˆ˜: {total_upside_down}íšŒ")
                print(f"   ì´ë²ˆ ë¡¤ì•„ì›ƒ ìƒˆë¡œìš´ ì‹œë„: {new_attempts}íšŒ")
                print(f"   í™˜ê²½ë‹¹ í‰ê· : {avg_per_env:.1f}íšŒ")
                
                # TensorBoardì—ë„ ë¡œê¹…
                if hasattr(self.logger, 'record'):
                    self.logger.record("custom/total_upside_down_attempts", total_upside_down)
                    self.logger.record("custom/new_upside_down_attempts", new_attempts)
                    self.logger.record("custom/avg_upside_down_per_env", avg_per_env)
                
                self.last_upside_down_count = total_upside_down
                
        except Exception as e:
            print(f"âš ï¸ ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_dir = Path("checkpoints") / f"{self.args.task}_training"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_path = checkpoint_dir / f"checkpoint_{self.num_timesteps}.zip"
        self.model.save(checkpoint_path)
        
        # âœ… ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ë„ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
        try:
            upside_down_count = getattr(self.eval_env.standing_reward, 'upside_down_count', 0)
        except:
            upside_down_count = 0
        
        metadata = {
            'timesteps': self.num_timesteps,
            'best_reward': self.best_reward,
            'upside_down_attempts': upside_down_count,  # âœ… ì¶”ê°€
            'args': vars(self.args)
        }
        
        import json
        with open(checkpoint_dir / f"metadata_{self.num_timesteps}.json", 'w') as f:
            json.dump(metadata, f, indent=2)
            
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path} (ë¬¼êµ¬ë‚˜ë¬´ ì‹œë„: {upside_down_count}íšŒ)")
    
    def _save_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        best_dir = Path("models") / "best"
        best_dir.mkdir(parents=True, exist_ok=True)
        
        # âœ… ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ í¬í•¨
        try:
            upside_down_count = getattr(self.eval_env.standing_reward, 'upside_down_count', 0)
            upside_down_info = f" (ë¬¼êµ¬ë‚˜ë¬´: {upside_down_count}íšŒ)"
        except:
            upside_down_info = ""
        
        best_path = best_dir / f"{self.args.task}_best_{self.num_timesteps}.zip"
        self.model.save(best_path)
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path} (ë³´ìƒ: {self.best_reward:.2f}){upside_down_info}")


def train_with_optimized_parameters(args):  
    """2ì¡± ë³´í–‰ ìµœì í™”ëœ í›ˆë ¨ - ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „"""
    print(f"\nğŸš€ 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì‹œì‘! (task={args.task})")
    print(f"ğŸ“Š ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"  - í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"  - ë¡¤ì•„ì›ƒ ìŠ¤í…: {args.n_steps}")
    print(f"  - í´ë¦½ ë²”ìœ„: {args.clip_range}")
    print(f"  - ì—”íŠ¸ë¡œí”¼: {args.entropy_coef}")
    print(f"  - ëª©í‘œ ì†ë„: {args.target_vel} m/s")
    print(f"  - ì•ˆì •ì„± ê°€ì¤‘ì¹˜: {args.stability_weight}")
    print(f"  - ë†’ì´ í—ˆìš©ì˜¤ì°¨: {args.height_tolerance}")
    print(f"  - ë³‘ë ¬ í™˜ê²½ ìˆ˜: {args.num_envs}")
    print(f"  - ì´ í›ˆë ¨ ìŠ¤í…: {args.total_timesteps:,}")
    print(f"  - ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: {'ì‚¬ìš©' if args.use_curriculum else 'ë¯¸ì‚¬ìš©'}")
    print(f"  - ì¡°ê¸° ì •ì§€: {'ì‚¬ìš©' if args.early_stopping else 'ë¯¸ì‚¬ìš©'}")
    
    # âœ… ìˆ˜ì •: í™˜ê²½ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨
    env_kwargs = {
        'randomize_physics': True,
    }
    
    # í™˜ê²½ ì„ íƒ
    if args.task == "standing":
        if args.use_curriculum:
            env_class = GradualStandingEnv
            print("ğŸ“š ì ì§„ì  ì»¤ë¦¬í˜ëŸ¼ í™˜ê²½ ì‚¬ìš©")
        else:
            env_class = BipedalWalkingEnv
            print("ğŸ¯ 2ì¡± ë³´í–‰ í™˜ê²½ ì‚¬ìš©")
    else:
        env_class = Go1MujocoEnv
        print("ğŸ• ê¸°ë³¸ 4ì¡± ë³´í–‰ í™˜ê²½ ì‚¬ìš©")
    
    # í•™ìŠµìš© í™˜ê²½ (ë³‘ë ¬í™”)
    print(f"\nğŸ­ {args.num_envs}ê°œ ë³‘ë ¬ í™˜ê²½ ìƒì„± ì¤‘...")
    vec_env = make_vec_env(
        env_class, 
        n_envs=args.num_envs, 
        vec_env_cls=SubprocVecEnv,
        env_kwargs=env_kwargs
    )
    
    # í‰ê°€ìš© í™˜ê²½
    print("ğŸ“Š í‰ê°€ í™˜ê²½ ìƒì„± ì¤‘...")
    eval_env = env_class(render_mode="rgb_array", **env_kwargs)
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EnhancedVisualCallback(
            eval_env,
            eval_interval_minutes=args.visual_interval,
            n_eval_episodes=3,
            show_duration_seconds=args.show_duration,
            save_videos=args.save_videos,
            use_curriculum=args.use_curriculum
        ),
        StandingTrainingCallback(args, eval_env)  # âœ… ê°œì„ ëœ ì½œë°± ì‚¬ìš©
    ]
    
    # ë¹„ë””ì˜¤ ë…¹í™” ì½œë°±
    if args.video_interval > 0:
        record_env = DummyVecEnv([lambda: env_class(render_mode="rgb_array", **env_kwargs)])
        callbacks.append(
            VideoRecordingCallback(
                record_env,
                record_interval_timesteps=args.video_interval,
                video_folder=f"eval_videos_{args.task}",
                show_duration_seconds=args.show_duration
            )
        )
    
    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    tensorboard_log = f"logs/{args.task}_optimized_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if args.pretrained_model:
        modelPath = args.pretrained_model
        pretrained_model = modelPath
        if modelPath == "latest":
            models = glob.glob(f"./models/{args.task}*.zip")
            pretrained_model = list(sorted(models))[-1]
        elif os.path.exists(args.pretrained_model):
            pretrained_model = args.pretrained_model
        
        print(f"ğŸ“‚ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ: {pretrained_model}")
        model = PPO.load(pretrained_model, env=vec_env)
        model.set_env(vec_env)
        
        # âœ… ìˆ˜ì •: clip_range í•¨ìˆ˜ë¡œ ì„¤ì • (float ì˜¤ë¥˜ í•´ê²°)
        if hasattr(model, 'learning_rate'):
            if args.use_curriculum:
                # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
                def lr_schedule(progress_remaining):
                    if progress_remaining > 0.9:
                        return args.learning_rate * 1.2
                    elif progress_remaining > 0.7:
                        return args.learning_rate
                    elif progress_remaining > 0.3:
                        return args.learning_rate * 0.5
                    else:
                        return args.learning_rate * 0.2
                model.learning_rate = lr_schedule
            else:
                model.learning_rate = args.learning_rate
        
        # âœ… ìˆ˜ì •: clip_rangeë¥¼ í•¨ìˆ˜ë¡œ ì„¤ì •
        if hasattr(model, 'clip_range'):
            def clip_range_func(progress_remaining):
                return args.clip_range
            model.clip_range = clip_range_func
        
        print(f"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
    else:
        print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_optimized_ppo_model(vec_env, args, tensorboard_log)
    
    # âœ… ìˆ˜ì •: training_time ì´ˆê¸°í™”
    training_time = 0.0
    
    # í•™ìŠµ ì‹œì‘
    try:
        print(f"\nğŸ¯ 2ì¡± ë³´í–‰ ìµœì í™” í•™ìŠµ ì‹œì‘...")
        print(f"ğŸ“Š TensorBoard ë¡œê·¸: {tensorboard_log}")
        print("ğŸ’¡ TensorBoard ì‹¤í–‰: tensorboard --logdir=logs")
        print("ğŸ“ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ ë³„ë„ í„°ë¯¸ë„ì—ì„œ TensorBoardë¥¼ ì‹¤í–‰í•˜ì„¸ìš”\n")
        
        start_time = time.time()
        
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=False if args.pretrained_model else True
        )
        
        training_time = time.time() - start_time
        print(f"\nâ±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        
    except KeyboardInterrupt:
        training_time = time.time() - start_time if 'start_time' in locals() else 0.0
        print(f"\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ - í˜„ì¬ ìƒíƒœ ì €ì¥ ì¤‘... (ì§„í–‰ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„)")
    except Exception as e:
        training_time = time.time() - start_time if 'start_time' in locals() else 0.0
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"â±ï¸ ì§„í–‰ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        import traceback
        traceback.print_exc()
    
    # ìµœì¢… ì €ì¥ ë° ë¶„ì„
    print("\nğŸ’¾ ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ë³´ê³ ì„œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"training_reports_{args.task}_optimized_{timestamp}"
    os.makedirs(report_path, exist_ok=True)
    
    if len(callbacks) > 0 and hasattr(callbacks[0], 'save_progress_report'):
        callbacks[0].save_progress_report(report_path)
        if hasattr(callbacks[0], 'save_detailed_analysis'):
            callbacks[0].save_detailed_analysis(report_path)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    model_path = f"models/{args.task}_optimized_final_{timestamp}.zip"
    Path("models").mkdir(exist_ok=True)
    model.save(model_path)
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")
    
    # âœ… ìˆ˜ì •: training_time ì•ˆì „í•˜ê²Œ ì‚¬ìš©
    config_path = os.path.join(report_path, "optimized_training_config.txt")
    with open(config_path, 'w') as f:
        f.write("=== 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì„¤ì • ===\n\n")
        f.write(f"Task: {args.task}\n")
        f.write(f"Total timesteps: {args.total_timesteps:,}\n")
        f.write(f"Training time: {training_time/3600:.2f} hours\n\n")
        
        f.write("=== í™˜ê²½ ì„¤ì • ===\n")
        f.write(f"Num environments: {args.num_envs}\n")
        f.write(f"Curriculum learning: {args.use_curriculum}\n")
        f.write(f"Target velocity: {args.target_vel} m/s (ì„¤ì •ê°’, í™˜ê²½ ë‚´ë¶€ì—ì„œ 0.0 ì‚¬ìš©)\n")
        f.write(f"Stability weight: {args.stability_weight} (ì„¤ì •ê°’, í™˜ê²½ ë‚´ë¶€ì—ì„œ ê³ ì •ê°’ ì‚¬ìš©)\n")
        f.write(f"Height tolerance: {args.height_tolerance} (ì„¤ì •ê°’, í™˜ê²½ ë‚´ë¶€ì—ì„œ 0.15 ì‚¬ìš©)\n\n")
        
        f.write("=== PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ===\n")
        f.write(f"Learning rate: {args.learning_rate}\n")
        f.write(f"Batch size: {args.batch_size}\n")
        f.write(f"N steps: {args.n_steps}\n")
        f.write(f"Clip range: {args.clip_range}\n")
        f.write(f"Entropy coefficient: {args.entropy_coef}\n")
        f.write(f"Early stopping: {args.early_stopping}\n\n")
        
        f.write("=== íŒŒì¼ ê²½ë¡œ ===\n")
        f.write(f"Final model: {model_path}\n")
        f.write(f"TensorBoard logs: {tensorboard_log}\n")
        f.write(f"Training reports: {report_path}\n")
        
        # âœ… ìˆ˜ì •: ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ì •ë³´ ì¶”ê°€
        if args.pretrained_model:
            f.write(f"Pretrained model: {args.pretrained_model}\n")
            f.write("Note: clip_range was converted to function to fix compatibility\n")
    
    # ìµœì¢… í‰ê°€
    print(f"\nğŸ§ª ìµœì¢… ëª¨ë¸ í‰ê°€ ì¤‘...")
    try:
        final_rewards = []
        final_successes = []
        
        for i in range(5):  # 5íšŒ í‰ê°€
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            
            for _ in range(1000):  # ìµœëŒ€ 1000 ìŠ¤í…
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if terminated or truncated:
                    break
            
            final_rewards.append(episode_reward)
            final_successes.append(info.get('standing_success', False))
            print(f"  í‰ê°€ {i+1}: ë³´ìƒ={episode_reward:.2f}, ê¸¸ì´={episode_length}, ì„±ê³µ={info.get('standing_success', False)}")
        
        mean_reward = np.mean(final_rewards)
        success_rate = np.mean(final_successes)
        
        print(f"\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
        print(f"  í‰ê·  ë³´ìƒ: {mean_reward:.2f} Â± {np.std(final_rewards):.2f}")
        print(f"  ì„±ê³µë¥ : {success_rate:.1%}")
        
        # ê²°ê³¼ë¥¼ config íŒŒì¼ì— ì¶”ê°€
        with open(config_path, 'a') as f:
            f.write(f"\n=== ìµœì¢… í‰ê°€ ê²°ê³¼ ===\n")
            f.write(f"Mean reward: {mean_reward:.2f} Â± {np.std(final_rewards):.2f}\n")
            f.write(f"Success rate: {success_rate:.1%}\n")
            
    except Exception as e:
        print(f"âš ï¸ ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {e}")
    
    # ì •ë¦¬
    eval_env.close()
    vec_env.close()
    if 'record_env' in locals():
        record_env.close()
    
    print(f"\nğŸ‰ 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {report_path}")
    print(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. TensorBoard í™•ì¸: tensorboard --logdir=logs")
    print(f"   2. ë³´ê³ ì„œ í™•ì¸: {report_path}")
    print(f"   3. ëª¨ë¸ í…ŒìŠ¤íŠ¸: python test_model.py --model {model_path}")
    print(f"   4. ë¹„ë””ì˜¤ í™•ì¸: eval_videos_{args.task}/")
    
    if args.use_curriculum:
        print(f"   5. ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ìƒí™©ì€ TensorBoardì—ì„œ í™•ì¸í•˜ì„¸ìš”")



if __name__ == "__main__":
    args = parse_arguments()
    train_with_optimized_parameters(args)