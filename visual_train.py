#!/usr/bin/env python3
import time
import threading
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv
import gymnasium as gym
from collections import deque
import os
import argparse
import subprocess
import traceback
import sys
import copy
import imageio.v2 as imageio
from datetime import datetime
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
from go1_mujoco_env import Go1MujocoEnv
import pandas as pd
from visual_training_callback import VisualTrainingCallback, VideoRecordingCallback, EnhancedVisualCallback
import torch
import glob
from collections import deque, defaultdict
try:
    from go1_standing_env import Go1StandingEnv, GradualStandingEnv, BipedalWalkingEnv, BipedalCurriculumEnv, RobotPhysicsUtils
except ImportError:
    print("âš ï¸ go1_standing_env.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
    raise

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    plt.rc('font', family='Malgun Gothic')
except:
    print("Malgun Gothic í°íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")
plt.rcParams['axes.unicode_minus'] = False


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹± - 2ì¡± ë³´í–‰ ìµœì í™”"""
    parser = argparse.ArgumentParser(description='2ì¡± ë³´í–‰ ê°•í™”í•™ìŠµ ì‹œê°ì  í›ˆë ¨')
    
    parser.add_argument('--extreme_gpu', action='store_true',
                       help='GPU í™œìš©ì„ ê·¹ëŒ€í™”í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ìµœì í™” ì ìš©')

    parser.add_argument('--task', type=str, default='standing', 
                       help='í›ˆë ¨í•  íƒœìŠ¤í¬ (ê¸°ë³¸ê°’: standing)')
    parser.add_argument('--pretrained_model', type=str, default=None,
                       help='ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--visual_interval', type=float, default=1.0,
                       help='ì‹œê°í™” ì£¼ê¸° (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0)')
    parser.add_argument('--show_duration', type=int, default=15,
                       help='ì‹œë®¬ë ˆì´ì…˜ ë³´ì—¬ì£¼ëŠ” ì‹œê°„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 15)')
    parser.add_argument('--save_videos', action='store_true',
                       help='ë¹„ë””ì˜¤ ì €ì¥ ì—¬ë¶€')
    parser.add_argument('--total_timesteps', type=int, default=5_000_000,
                       help='ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 5,000,000)')
    parser.add_argument('--num_envs', type=int, default=12,
                       help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (ê¸°ë³¸ê°’: 12)')
    parser.add_argument('--video_interval', type=int, default=150_000,
                       help='ë¹„ë””ì˜¤ ë…¹í™” ê°„ê²© (timesteps, ê¸°ë³¸ê°’: 150,000)')
    parser.add_argument('--use_curriculum', action='store_true',
                       help='ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‚¬ìš©')
    
    parser.add_argument('--learning_rate', type=float, default=2e-5,
                       help='í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-4)')
    parser.add_argument('--batch_size', type=int, default=128,
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 128)')
    parser.add_argument('--n_steps', type=int, default=1024,
                       help='ë¡¤ì•„ì›ƒ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 1024)')
    parser.add_argument('--clip_range', type=float, default=0.15,
                       help='PPO í´ë¦½ ë²”ìœ„ (ê¸°ë³¸ê°’: 0.15)')
    parser.add_argument('--entropy_coef', type=float, default=0.005,
                       help='ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (ê¸°ë³¸ê°’: 0.005)')
    
    parser.add_argument('--target_vel', type=float, default=0.0,
                       help='ëª©í‘œ ì†ë„ (ê¸°ë³¸ê°’: 0.0 - ì œìë¦¬ ì„œê¸°)')
    parser.add_argument('--stability_weight', type=float, default=1.5,
                       help='ì•ˆì •ì„± ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.5)')
    parser.add_argument('--height_tolerance', type=float, default=0.12,
                       help='ë†’ì´ í—ˆìš© ì˜¤ì°¨ (ê¸°ë³¸ê°’: 0.12)')
    parser.add_argument('--early_stopping', action='store_true',
                       help='ì¡°ê¸° ì •ì§€ ì‚¬ìš© (ìˆ˜ë ´ ì‹œ)')
    parser.add_argument('--checkpoint_interval', type=int, default=500_000,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²© (ê¸°ë³¸ê°’: 500,000)')
    
    parser.add_argument('--ignore_pretrained_obs_mismatch', action='store_true',
                       help='ì‚¬ì „í›ˆë ¨ ëª¨ë¸ê³¼ ê´€ì°°ê³µê°„ ë¶ˆì¼ì¹˜ ë¬´ì‹œí•˜ê³  ìƒˆ ëª¨ë¸ ìƒì„±')
    
    parser.add_argument('--randomness_intensity', type=float, default=1.5,
                       help='í›ˆë ¨ ì‹œ ëœë¤ì„± ê°•ë„ (0.0=ì—†ìŒ, 1.0=ê¸°ë³¸, 2.0=ê°•í™”, ê¸°ë³¸ê°’: 1.5)')
    
    return parser.parse_args()


def check_observation_compatibility(pretrained_model_path, current_env):
    """ì‚¬ì „í›ˆë ¨ ëª¨ë¸ê³¼ í˜„ì¬ í™˜ê²½ì˜ ê´€ì°° ê³µê°„ í˜¸í™˜ì„± í™•ì¸"""
    try:
        if os.path.exists(pretrained_model_path):
            # --- ìˆ˜ì •: í˜¸í™˜ì„± í™•ì¸ ë¡œì§ ê°„ì†Œí™” ---
            # ì—¬ê¸°ì„œ ë°œìƒí•˜ëŠ” state_dict ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³ , ì‹¤ì œ ë¡œë“œ ì‹œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
            # ì—¬ê¸°ì„œëŠ” ê´€ì°° ê³µê°„ í¬ê¸°ë§Œ ë¹„êµí•˜ëŠ” ê²ƒì´ ëª©ì 
            from stable_baselines3.common.save_util import load_from_zip_file
            data, params, pytorch_variables = load_from_zip_file(pretrained_model_path)
            
            # ëª¨ë¸ì˜ ê´€ì°° ê³µê°„ shape ì¶”ì •
            # policy_kwargsê°€ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
            if params and 'policy' in params and 'observation_space' in params['policy']:
                 model_obs_shape = params['policy']['observation_space'].shape
            else:
                # ì—†ë‹¤ë©´, state_dictì—ì„œ ì²« ë²ˆì§¸ ë ˆì´ì–´ í¬ê¸°ë¡œ ì¶”ì • (ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ)
                first_weight_key = next(iter(data['policy']))
                first_weight_tensor = data['policy'][first_weight_key]
                # This is a heuristic and might not always be correct
                model_obs_shape = (first_weight_tensor.shape[1],) if len(first_weight_tensor.shape) > 1 else None

            current_obs_shape = current_env.observation_space.shape
            
            print(f"ğŸ” ê´€ì°° ê³µê°„ í˜¸í™˜ì„± í™•ì¸:")
            print(f"  ì‚¬ì „í›ˆë ¨ ëª¨ë¸ (ì¶”ì •): {model_obs_shape}")
            print(f"  í˜„ì¬ í™˜ê²½: {current_obs_shape}")

            if model_obs_shape is None or model_obs_shape[0] != current_obs_shape[0]:
                 print("âŒ ê´€ì°° ê³µê°„ ë¶ˆì¼ì¹˜ ê°ì§€")
                 return False
            else:
                 print("âœ… ê´€ì°° ê³µê°„ í˜¸í™˜ ê°€ëŠ¥")
                 return True
            
    except Exception as e:
        print(f"âš ï¸ í˜¸í™˜ì„± í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¡œë“œ ì‹œ ì¬ì‹œë„): {e}")
        # í™•ì¸ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë‚˜ë”ë¼ë„, ì‹¤ì œ ë¡œë“œ ë¡œì§ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ Trueë¥¼ ë°˜í™˜í•˜ì—¬ ì§„í–‰
        return True
    
    return False


def create_optimized_ppo_model(env, args, device, tensorboard_log=None):
    """2ì¡± ë³´í–‰ ìµœì í™”ëœ PPO ëª¨ë¸ ìƒì„±"""
    
    if args.extreme_gpu:
        print("ğŸš€ ê·¹ë‹¨ì  GPU í™œìš© ëª¨ë“œë¡œ PPO ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        ppo_params = {
            'n_steps': 8192,
            'batch_size': 1024,
            'n_epochs': 60,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'learning_rate': 3e-5,
            'clip_range': 0.2,
            'ent_coef': 0.001,
            'vf_coef': 0.5,
            'max_grad_norm': 1.0,
            'policy_kwargs': dict(
                net_arch=[dict(pi=[1024, 512, 256], vf=[1024, 512, 256])],
                activation_fn=torch.nn.ReLU,
                ortho_init=True,
                log_std_init=-2.0
            ),
        }
    else:
        def standing_lr_schedule(progress_remaining):
            if progress_remaining > 0.8: return 3e-5
            elif progress_remaining > 0.5: return 2e-5
            else: return 1e-5
                
        def clip_range_schedule(progress_remaining):
            return 0.2 if progress_remaining > 0.5 else 0.1

        ppo_params = {
            'n_steps': 4096,
            'batch_size': 256,
            'n_epochs': 30,
            'gamma': 0.98,
            'gae_lambda': 0.95,
            'learning_rate': standing_lr_schedule,
            'clip_range': clip_range_schedule,
            'ent_coef': 0.005,
            'vf_coef': 0.7,
            'max_grad_norm': 0.5,
            'policy_kwargs': dict(
                net_arch=[dict(pi=[512, 256], vf=[512, 256])],
                activation_fn=torch.nn.ReLU,
                ortho_init=True,
                log_std_init=-2.0
            ),
        }

    model = PPO(
        "MlpPolicy",
        env,
        normalize_advantage=True,
        use_sde=False,
        tensorboard_log=tensorboard_log,
        verbose=1,
        device=device,
        **ppo_params
    )
    
    return model

# StandingTrainingCallback í´ë˜ìŠ¤ëŠ” ë³€ê²½ ì‚¬í•­ ì—†ìŒ (ìƒëµ)
class StandingTrainingCallback(BaseCallback):
    def __init__(self, args, eval_env, verbose=0):
        super().__init__(verbose)
        self.args = args
        self.eval_env = eval_env
        self.best_reward = -np.inf
        self.no_improvement_steps = 0
        self.patience = 1_000_000
        self.episode_rewards = deque(maxlen=100)
        self.success_rates = deque(maxlen=50)
        self.last_checkpoint = 0
        self.last_upside_down_count = 0
        self.manual_info_buffer = deque(maxlen=args.num_envs * 2)
    def _get_reward_object(self, env):
        if hasattr(env, 'bipedal_reward'): return env.bipedal_reward
        elif hasattr(env, 'standing_reward'): return env.standing_reward
        elif hasattr(env, 'env'):
            if hasattr(env.env, 'bipedal_reward'): return env.env.bipedal_reward
            elif hasattr(env.env, 'standing_reward'): return env.env.standing_reward
        return None
    def _on_step(self) -> bool:
        dones = self.locals.get("dones", [])
        infos = self.locals.get("infos", [])
        for i, done in enumerate(dones):
            if done: self.manual_info_buffer.append(copy.deepcopy(infos[i]))
        if (self.num_timesteps - self.last_checkpoint >= self.args.checkpoint_interval):
            self._save_checkpoint()
            self.last_checkpoint = self.num_timesteps
        return True
    def _on_rollout_end(self) -> bool:
        self.manual_info_buffer.clear()
        if len(self.locals.get('episode_rewards', [])) > 0:
            recent_rewards = self.locals['episode_rewards'][-10:]
            mean_reward = np.mean(recent_rewards)
            self.episode_rewards.extend(recent_rewards)
            if mean_reward > self.best_reward:
                self.best_reward = mean_reward
                self.no_improvement_steps = 0
                self._save_best_model()
            else:
                self.no_improvement_steps += self.args.n_steps * self.args.num_envs
        self._log_upside_down_statistics()
        if (self.args.early_stopping and self.no_improvement_steps > self.patience):
            print(f"\nğŸ›‘ ì¡°ê¸° ì •ì§€: {self.patience:,} ìŠ¤í… ë™ì•ˆ ê°œì„  ì—†ìŒ")
            return False
        return True
    def _log_upside_down_statistics(self):
        try:
            upside_down_counts = []
            if hasattr(self.training_env, 'envs'):
                for env in self.training_env.envs:
                    try:
                        reward_obj = self._get_reward_object(env)
                        if reward_obj: upside_down_counts.append(getattr(reward_obj, 'upside_down_count', 0))
                    except: pass
            if upside_down_counts:
                total_upside_down = sum(upside_down_counts)
                new_attempts = total_upside_down - self.last_upside_down_count
                avg_per_env = total_upside_down / len(upside_down_counts)
                print(f"ğŸš¨ ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„: ì´ {total_upside_down}íšŒ, ì´ë²ˆ ë¡¤ì•„ì›ƒ {new_attempts}íšŒ, í™˜ê²½ë‹¹ í‰ê·  {avg_per_env:.1f}íšŒ")
                if hasattr(self.logger, 'record'):
                    self.logger.record("custom/total_upside_down_attempts", total_upside_down)
                    self.logger.record("custom/new_upside_down_attempts", new_attempts)
                    self.logger.record("custom/avg_upside_down_per_env", avg_per_env)
                self.last_upside_down_count = total_upside_down
        except Exception as e: print(f"âš ï¸ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    def _save_checkpoint(self):
        checkpoint_dir = Path("checkpoints") / f"{self.args.task}_training"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_path = checkpoint_dir / f"checkpoint_{self.num_timesteps}.zip"
        self.model.save(checkpoint_path)
        try: reward_obj = self._get_reward_object(self.eval_env); upside_down_count = getattr(reward_obj, 'upside_down_count', 0) if reward_obj else 0
        except: upside_down_count = 0
        metadata = {'timesteps': self.num_timesteps, 'best_reward': self.best_reward, 'upside_down_attempts': upside_down_count, 'args': vars(self.args)}
        import json
        with open(checkpoint_dir / f"metadata_{self.num_timesteps}.json", 'w') as f: json.dump(metadata, f, indent=2)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path} (ë¬¼êµ¬ë‚˜ë¬´ ì‹œë„: {upside_down_count}íšŒ)")
    def _save_best_model(self):
        best_dir = Path("models") / "best"
        best_dir.mkdir(parents=True, exist_ok=True)
        try: reward_obj = self._get_reward_object(self.eval_env); upside_down_count = getattr(reward_obj, 'upside_down_count', 0) if reward_obj else 0; upside_down_info = f" (ë¬¼êµ¬ë‚˜ë¬´: {upside_down_count}íšŒ)"
        except: upside_down_info = ""
        best_path = best_dir / f"{self.args.task}_best_{self.num_timesteps}.zip"
        self.model.save(best_path)
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path} (ë³´ìƒ: {self.best_reward:.2f}){upside_down_info}")


def load_compiled_model(model_path, env, device):
    """torch.compileë¡œ ì €ì¥ëœ ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜"""
    print(f"ğŸ“¦ ì»´íŒŒì¼ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤: {model_path}")
    
    from stable_baselines3.common.save_util import load_from_zip_file
    
    # 1. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•©ë‹ˆë‹¤.
    data, params, pytorch_variables = load_from_zip_file(model_path, device=device)
    
    # 2. ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìƒˆë¡œìš´ PPO ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    #    ì´ë ‡ê²Œ í•˜ë©´ ì˜¬ë°”ë¥¸ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ê°€ì§„ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.
    model = PPO(
        policy=params["policy_class"],
        env=env,
        device=device,
        _init_setup_model=False, # ëª¨ë¸ì„ ë°”ë¡œ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
    )
    model.set_parameters(params, exact_match=False) # ë¡œë“œëœ íŒŒë¼ë¯¸í„° ì ìš©
    model._setup_model() # ëª¨ë¸ ì´ˆê¸°í™”

    # 3. state_dictì˜ í‚¤ì—ì„œ '_orig_mod.' ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    policy_state_dict = data['policy']
    cleaned_state_dict = {}
    for k, v in policy_state_dict.items():
        cleaned_state_dict[k.replace("_orig_mod.", "")] = v
        
    # 4. ì •ë¦¬ëœ state_dictë¥¼ ëª¨ë¸ ì •ì±…ì— ë¡œë“œí•©ë‹ˆë‹¤.
    model.policy.load_state_dict(cleaned_state_dict)
    
    print("âœ… ì»´íŒŒì¼ëœ ëª¨ë¸ ìƒíƒœ ë³µêµ¬ ë° ë¡œë“œ ì„±ê³µ!")
    return model


def train_with_optimized_parameters(args):
    """2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ - ê´€ì°° ê³µê°„ í˜¸í™˜ì„± ìë™ í™•ì¸ ë° ìˆ˜ì • ì ìš©"""
    
    if torch.cuda.is_available():
        print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥. GPU(RTX 5080)ë¡œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        print(f"   - PyTorch ë²„ì „: {torch.__version__}")
        print(f"   - CUDA ë²„ì „: {torch.version.cuda}")
        print(f"   - GPU ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
        device = torch.device("cuda")
        torch.set_float32_matmul_precision('high')
    else:
        print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        device = torch.device("cpu")

    if args.extreme_gpu:
        print("="*60)
        print("âš ï¸ 'ê·¹ë‹¨ì  GPU í™œìš© ëª¨ë“œ'ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   - ë³‘ë ¬ í™˜ê²½ ìˆ˜(--num_envs)ë¥¼ CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ë†’ì—¬ì£¼ì„¸ìš” (ì˜ˆ: 16, 24, 32).")
        print("   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ê²Œ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("="*60)

    print(f"\nğŸš€ 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì‹œì‘! (task={args.task})")
    
    training_time = 0.0
    randomness_intensity = args.randomness_intensity
    RobotPhysicsUtils.set_randomness_intensity(args.randomness_intensity)
    print(f"ğŸ›ï¸ ëœë¤ì„± ê°•ë„ ì„¤ì •: {args.randomness_intensity}")

    env_class = BipedalWalkingEnv if args.task == "standing" else Go1MujocoEnv
    env_kwargs = {'randomize_physics': True}
    print(f"ğŸ¯ í›ˆë ¨ í™˜ê²½: {env_class.__name__}")

    use_pretrained = False
    pretrained_model_path = args.pretrained_model
    if pretrained_model_path and os.path.exists(pretrained_model_path):
        use_pretrained = True

    print(f"\nğŸ­ {args.num_envs}ê°œ ë³‘ë ¬ í™˜ê²½ ìƒì„± ì¤‘...")
    vec_env = make_vec_env(env_class, n_envs=args.num_envs, vec_env_cls=SubprocVecEnv, env_kwargs=env_kwargs)
    eval_env = env_class(render_mode="rgb_array", **env_kwargs)

    callbacks = [
        EnhancedVisualCallback(eval_env, eval_interval_minutes=args.visual_interval, n_eval_episodes=3, show_duration_seconds=args.show_duration, save_videos=args.save_videos),
        StandingTrainingCallback(args, eval_env)
    ]
    if args.video_interval > 0:
        record_env = DummyVecEnv([lambda: env_class(render_mode="rgb_array", **env_kwargs)])
        callbacks.append(
            VideoRecordingCallback(record_env, record_interval_timesteps=args.video_interval, video_folder=f"eval_videos_{args.task}", show_duration_seconds=args.show_duration)
        )

    tensorboard_log = f"logs/{args.task}_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if use_pretrained:
        print(f"ğŸ“‚ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤: {pretrained_model_path}")
        try:
            # --- ìˆ˜ì •: torch.compile ë³µêµ¬ ë¡œì§ ì ìš© ---
            # ë¨¼ì € ì¼ë°˜ ë¡œë“œë¥¼ ì‹œë„í•˜ê³ , ì‹¤íŒ¨í•˜ë©´ ì»´íŒŒì¼ëœ ëª¨ë¸ ë³µêµ¬ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
            try:
                model = PPO.load(pretrained_model_path, env=vec_env, device=device)
                print("âœ… ëª¨ë¸ ì¼ë°˜ ë¡œë“œ ì„±ê³µ.")
            except Exception:
                model = load_compiled_model(pretrained_model_path, vec_env, device)
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ë¡œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            model = create_optimized_ppo_model(vec_env, args, device, tensorboard_log)
    else:
        print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_optimized_ppo_model(vec_env, args, device, tensorboard_log)
    
    if args.extreme_gpu and device.type == 'cuda' and hasattr(torch, 'compile'):
        print("ğŸš€ PyTorch 2.x JIT ì»´íŒŒì¼ëŸ¬(torch.compile)ë¥¼ ì •ì±… ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
        try:
            model.policy = torch.compile(model.policy)
            print("âœ… torch.compile ì ìš© ì„±ê³µ!")
        except Exception as e:
            print(f"âš ï¸ torch.compile ì ìš© ì‹¤íŒ¨. Tritonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í™˜ê²½(ì˜ˆ: Windows)ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print(f"   (ì˜¤ë¥˜: {e})")
            print(f"   JIT ì»´íŒŒì¼ ì—†ì´ í›ˆë ¨ì„ ê³„ì†í•©ë‹ˆë‹¤.")

    try:
        start_time = time.time()
        print(f"\nğŸ¯ í•™ìŠµ ì‹œì‘...")
        # reset_num_timesteps=Falseë¡œ ì„¤ì •í•˜ì—¬ ì´ì „ í•™ìŠµ ìŠ¤í…ì„ ì´ì–´ê°€ë„ë¡ í•¨
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=not use_pretrained 
        )
        training_time = time.time() - start_time
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()

    # ìµœì¢… ì €ì¥ ë° ë¶„ì„ (ìƒëµ)
    print("\nğŸ’¾ ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì¤‘...")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"training_reports_{args.task}_optimized_{timestamp}"
    os.makedirs(report_path, exist_ok=True)
    
    model_path = f"models/{args.task}_optimized_final_{timestamp}.zip"
    Path("models").mkdir(exist_ok=True)
    model.save(model_path)
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")
    
    # ì„¤ì • ì €ì¥
    config_path = os.path.join(report_path, "optimized_training_config.txt")
    with open(config_path, 'w') as f:
        f.write("=== 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì„¤ì • ===\n\n")
        f.write(f"Task: {args.task}\n")
        f.write(f"Total timesteps: {args.total_timesteps:,}\n")
        f.write(f"Training time: {training_time/3600:.2f} hours\n")
        f.write(f"Used pretrained model: {use_pretrained}\n")
        f.write(f"Randomness intensity: {randomness_intensity}\n")  # âœ… ëœë¤ì„± ê°•ë„ ê¸°ë¡
        if use_pretrained:
            f.write(f"Pretrained model path: {pretrained_model_path}\n")
        f.write(f"Environment observation mode: {'Base(45dim)' if env_kwargs.get('use_base_observation', False) else 'Extended(56dim)'}\n")
        f.write("\n")
        
        f.write("=== í™˜ê²½ ì„¤ì • ===\n")
        f.write(f"Environment class: {env_class.__name__}\n")
        f.write(f"Num environments: {args.num_envs}\n")
        f.write(f"Curriculum learning: {args.use_curriculum}\n")
        f.write(f"Target velocity: {args.target_vel} m/s\n")
        f.write(f"Stability weight: {args.stability_weight}\n")
        f.write(f"Height tolerance: {args.height_tolerance}\n")
        f.write(f"Use base observation: {env_kwargs.get('use_base_observation', False)}\n\n")
        
        f.write("=== PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ===\n")
        f.write(f"Learning rate: {args.learning_rate}\n")
        f.write(f"Batch size: {args.batch_size}\n")
        f.write(f"N steps: {args.n_steps}\n")
        f.write(f"Clip range: {args.clip_range}\n")
        f.write(f"Entropy coefficient: {args.entropy_coef}\n")
        f.write(f"Early stopping: {args.early_stopping}\n\n")
        
        f.write("=== íŒŒì¼ ê²½ë¡œ ===\n")
        f.write(f"Final model: {model_path}\n")
        f.write(f"TensorBoard logs: {tensorboard_log}\n")
        f.write(f"Training reports: {report_path}\n")
        
        if use_pretrained:
            f.write(f"Original pretrained model: {args.pretrained_model}\n")
            f.write(f"Observation compatibility: {'Compatible' if use_pretrained else 'Incompatible - created new model'}\n")
   
    # ìµœì¢… í‰ê°€
    print(f"\nğŸ§ª ìµœì¢… ëª¨ë¸ í‰ê°€ ì¤‘...")
    try:
        final_rewards = []
        final_successes = []
        
        for i in range(5):  # 5íšŒ í‰ê°€
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            
            for _ in range(1000):  # ìµœëŒ€ 1000 ìŠ¤í…
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if terminated or truncated:
                    break
            
            final_rewards.append(episode_reward)
            # í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ ì„±ê³µ í‚¤ ì‚¬ìš©
            if hasattr(eval_env, 'bipedal_reward'):
                success_key = 'bipedal_success'
            else:
                success_key = 'standing_success'
            final_successes.append(info.get(success_key, False))
            print(f"  í‰ê°€ {i+1}: ë³´ìƒ={episode_reward:.2f}, ê¸¸ì´={episode_length}, ì„±ê³µ={info.get(success_key, False)}")
        
        mean_reward = np.mean(final_rewards)
        success_rate = np.mean(final_successes)
        
        print(f"\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
        print(f"  í‰ê·  ë³´ìƒ: {mean_reward:.2f} Â± {np.std(final_rewards):.2f}")
        print(f"  ì„±ê³µë¥ : {success_rate:.1%}")
        
        # ê²°ê³¼ë¥¼ config íŒŒì¼ì— ì¶”ê°€
        with open(config_path, 'a') as f:
            f.write(f"\n=== ìµœì¢… í‰ê°€ ê²°ê³¼ ===\n")
            f.write(f"Mean reward: {mean_reward:.2f} Â± {np.std(final_rewards):.2f}\n")
            f.write(f"Success rate: {success_rate:.1%}\n")
            
    except Exception as e:
        print(f"âš ï¸ ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {e}")
    
    # ì •ë¦¬
    eval_env.close()
    vec_env.close()
    if 'record_env' in locals():
        record_env.close()
    
    print(f"\nğŸ‰ 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {report_path}")
    print(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. TensorBoard í™•ì¸: tensorboard --logdir=logs")
    print(f"   2. ë³´ê³ ì„œ í™•ì¸: {report_path}")
    print(f"   3. ëª¨ë¸ í…ŒìŠ¤íŠ¸: python test_model.py --model {model_path}")
    print(f"   4. ë¹„ë””ì˜¤ í™•ì¸: eval_videos_{args.task}/")
    
    # âœ… [ìˆ˜ì •] --use_curriculum í”Œë˜ê·¸ì— ë”°ë¼ í™˜ê²½ì„ ì„ íƒí•˜ë„ë¡ ë³€ê²½
    if args.use_curriculum:
        # ì»¤ë¦¬í˜ëŸ¼ í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ BipedalCurriculumEnv ì‚¬ìš©
        env_class = BipedalCurriculumEnv 
        print("ğŸ“ ì»¤ë¦¬í˜ëŸ¼ ëª¨ë“œë¡œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤. (BipedalCurriculumEnv)")
    else:
        # ê¸°ë³¸ ëª¨ë“œ
        env_class = BipedalWalkingEnv if args.task == "standing" else Go1MujocoEnv

    env_kwargs = {'randomize_physics': True}
    print(f"ğŸ¯ í›ˆë ¨ í™˜ê²½: {env_class.__name__}")
    
    if not use_pretrained and args.pretrained_model:
        print(f"\nğŸ’¡ ì°¸ê³ : ê´€ì°° ê³µê°„ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ìƒˆ ëª¨ë¸ë¡œ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"   í˜¸í™˜ ê°€ëŠ¥í•œ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´:")
        print(f"   1. ê°™ì€ í™˜ê²½ í´ë˜ìŠ¤ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©")
        print(f"   2. ë˜ëŠ” --ignore_pretrained_obs_mismatch í”Œë˜ê·¸ ì‚¬ìš©")
        print(f"   3. ë˜ëŠ” í™˜ê²½ì— use_base_observation=True ì„¤ì •")

if __name__ == "__main__":
    args = parse_arguments()
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    train_with_optimized_parameters(args)
