#!/usr/bin/env python3
import time
import threading
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv
import gymnasium as gym
from collections import deque
import os
import argparse
import subprocess
import sys
import copy
import imageio.v2 as imageio
from datetime import datetime
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
from go1_mujoco_env import Go1MujocoEnv
import pandas as pd
from visual_training_callback import VisualTrainingCallback, VideoRecordingCallback, EnhancedVisualCallback
import torch
import glob
from collections import deque, defaultdict
try:
    from go1_standing_env import Go1StandingEnv, GradualStandingEnv, BipedalWalkingEnv, RobotPhysicsUtils
except ImportError:
    print("âš ï¸ go1_standing_env.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤!")
    raise

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    plt.rc('font', family='Malgun Gothic')
except:
    print("Malgun Gothic í°íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")
plt.rcParams['axes.unicode_minus'] = False


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹± - 2ì¡± ë³´í–‰ ìµœì í™”"""
    parser = argparse.ArgumentParser(description='2ì¡± ë³´í–‰ ê°•í™”í•™ìŠµ ì‹œê°ì  í›ˆë ¨')
    
    parser.add_argument('--task', type=str, default='standing', 
                       help='í›ˆë ¨í•  íƒœìŠ¤í¬ (ê¸°ë³¸ê°’: standing)')
    parser.add_argument('--pretrained_model', type=str, default=None,
                       help='ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--visual_interval', type=float, default=1.0,
                       help='ì‹œê°í™” ì£¼ê¸° (ë¶„ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0)')
    parser.add_argument('--show_duration', type=int, default=15,
                       help='ì‹œë®¬ë ˆì´ì…˜ ë³´ì—¬ì£¼ëŠ” ì‹œê°„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 15)')
    parser.add_argument('--save_videos', action='store_true',
                       help='ë¹„ë””ì˜¤ ì €ì¥ ì—¬ë¶€')
    parser.add_argument('--total_timesteps', type=int, default=5_000_000,
                       help='ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 5,000,000)')
    parser.add_argument('--num_envs', type=int, default=12,
                       help='ë³‘ë ¬ í™˜ê²½ ìˆ˜ (ê¸°ë³¸ê°’: 12)')
    parser.add_argument('--video_interval', type=int, default=150_000,
                       help='ë¹„ë””ì˜¤ ë…¹í™” ê°„ê²© (timesteps, ê¸°ë³¸ê°’: 150,000)')
    parser.add_argument('--use_curriculum', action='store_true',
                       help='ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‚¬ìš©')
    
    # 2ì¡± ë³´í–‰ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--learning_rate', type=float, default=2e-4,
                       help='í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-4)')
    parser.add_argument('--batch_size', type=int, default=128,
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 128)')
    parser.add_argument('--n_steps', type=int, default=1024,
                       help='ë¡¤ì•„ì›ƒ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 1024)')
    parser.add_argument('--clip_range', type=float, default=0.15,
                       help='PPO í´ë¦½ ë²”ìœ„ (ê¸°ë³¸ê°’: 0.15)')
    parser.add_argument('--entropy_coef', type=float, default=0.005,
                       help='ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (ê¸°ë³¸ê°’: 0.005)')
    
    # ìƒˆë¡œìš´ 2ì¡± ë³´í–‰ íŠ¹í™” íŒŒë¼ë¯¸í„°
    parser.add_argument('--target_vel', type=float, default=0.0,
                       help='ëª©í‘œ ì†ë„ (ê¸°ë³¸ê°’: 0.0 - ì œìë¦¬ ì„œê¸°)')
    parser.add_argument('--stability_weight', type=float, default=1.5,
                       help='ì•ˆì •ì„± ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.5)')
    parser.add_argument('--height_tolerance', type=float, default=0.12,
                       help='ë†’ì´ í—ˆìš© ì˜¤ì°¨ (ê¸°ë³¸ê°’: 0.12)')
    parser.add_argument('--early_stopping', action='store_true',
                       help='ì¡°ê¸° ì •ì§€ ì‚¬ìš© (ìˆ˜ë ´ ì‹œ)')
    parser.add_argument('--checkpoint_interval', type=int, default=500_000,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²© (ê¸°ë³¸ê°’: 500,000)')
    
    # ìƒˆë¡œìš´ ì˜µì…˜: ê´€ì°° ê³µê°„ í˜¸í™˜ì„±
    parser.add_argument('--ignore_pretrained_obs_mismatch', action='store_true',
                       help='ì‚¬ì „í›ˆë ¨ ëª¨ë¸ê³¼ ê´€ì°°ê³µê°„ ë¶ˆì¼ì¹˜ ë¬´ì‹œí•˜ê³  ìƒˆ ëª¨ë¸ ìƒì„±')
    
    # âœ… ëœë¤ì„± ê°•ë„ ì¡°ì • ì˜µì…˜ ì¶”ê°€
    parser.add_argument('--randomness_intensity', type=float, default=1.5,
                       help='í›ˆë ¨ ì‹œ ëœë¤ì„± ê°•ë„ (0.0=ì—†ìŒ, 1.0=ê¸°ë³¸, 2.0=ê°•í™”, ê¸°ë³¸ê°’: 1.5)')
    
    return parser.parse_args()


def check_observation_compatibility(pretrained_model_path, current_env):
    """ì‚¬ì „í›ˆë ¨ ëª¨ë¸ê³¼ í˜„ì¬ í™˜ê²½ì˜ ê´€ì°° ê³µê°„ í˜¸í™˜ì„± í™•ì¸"""
    try:
        # ì„ì‹œë¡œ ëª¨ë¸ ë¡œë“œí•´ì„œ observation space í™•ì¸
        if os.path.exists(pretrained_model_path):
            temp_model = PPO.load(pretrained_model_path, env=None)
            
            # ëª¨ë¸ì˜ observation space ì¶”ì¶œ
            if hasattr(temp_model.policy, 'observation_space'):
                model_obs_shape = temp_model.policy.observation_space.shape
            else:
                # ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ í¬ê¸°ë¡œ ì¶”ì •
                first_layer = next(temp_model.policy.features_extractor.parameters())
                model_obs_shape = (first_layer.shape[1],)
            
            # í˜„ì¬ í™˜ê²½ì˜ observation space
            current_obs_shape = current_env.observation_space.shape
            
            print(f"ğŸ” ê´€ì°° ê³µê°„ í˜¸í™˜ì„± í™•ì¸:")
            print(f"  ì‚¬ì „í›ˆë ¨ ëª¨ë¸: {model_obs_shape}")
            print(f"  í˜„ì¬ í™˜ê²½: {current_obs_shape}")
            
            compatible = model_obs_shape == current_obs_shape
            
            if compatible:
                print("âœ… ê´€ì°° ê³µê°„ í˜¸í™˜ ê°€ëŠ¥")
            else:
                print("âŒ ê´€ì°° ê³µê°„ ë¶ˆì¼ì¹˜ ê°ì§€")
                print("  ì˜µì…˜:")
                print("  1. --ignore_pretrained_obs_mismatch í”Œë˜ê·¸ ì‚¬ìš©")
                print("  2. ë™ì¼í•œ í™˜ê²½ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©")
                print("  3. ìƒˆë¡œìš´ ëª¨ë¸ë¡œ í›ˆë ¨ ì‹œì‘")
            
            del temp_model  # ë©”ëª¨ë¦¬ ì •ë¦¬
            return compatible
            
    except Exception as e:
        print(f"âš ï¸ í˜¸í™˜ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
        return False
    
    return False


def create_optimized_ppo_model(env, args, tensorboard_log=None):
    """2ì¡± ë³´í–‰ ìµœì í™”ëœ PPO ëª¨ë¸ ìƒì„±"""
    
    def standing_lr_schedule(progress_remaining):
        if progress_remaining > 0.8:
            return 1e-4
        elif progress_remaining > 0.5:
            return 5e-5
        else:
            return 1e-5
            
    def clip_range_schedule(progress_remaining):
        if progress_remaining > 0.5:
            return 0.2
        else:
            return 0.1

    lr_schedule = standing_lr_schedule
    clip_range = clip_range_schedule
    
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=lr_schedule,
        n_steps=4096,
        batch_size=256,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=clip_range,
        ent_coef=0.005,             # âœ… [ìˆ˜ì •] ì´ˆê¸° íƒí—˜ì„ ì¥ë ¤í•˜ê¸° ìœ„í•´ ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ ì•½ê°„ ì¦ê°€ (ê¸°ì¡´ 0.001)
        vf_coef=0.5,
        max_grad_norm=0.5,
        use_sde=False,
        tensorboard_log=tensorboard_log,
        verbose=1,
        policy_kwargs=dict(
            net_arch=[dict(pi=[512, 256], vf=[512, 256])],
            activation_fn=torch.nn.ReLU,
            ortho_init=True,
            log_std_init=-2.0
        ),
        device='auto'
    )
    
    return model


class StandingTrainingCallback(BaseCallback):
    """2ì¡± ë³´í–‰ íŠ¹í™” í›ˆë ¨ ì½œë°± - í™˜ê²½ë³„ ë³´ìƒ ê°ì²´ í˜¸í™˜"""
    
    def __init__(self, args, eval_env, verbose=0):
        super().__init__(verbose)
        self.args = args
        self.eval_env = eval_env
        self.best_reward = -np.inf
        self.no_improvement_steps = 0
        self.patience = 1_000_000
        
        # ì„±ëŠ¥ ì¶”ì 
        self.episode_rewards = deque(maxlen=100)
        self.success_rates = deque(maxlen=50)
        self.last_checkpoint = 0
        
        # ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„ ì¶”ì 
        self.last_upside_down_count = 0
        
        # âš ï¸ [ì œê±°] ì „ì—­ ì¢…ë£Œ ì›ì¸ í†µê³„ ì¶”ì  ì œê±°
        # self.termination_counts = defaultdict(int)

        # ì •ë³´ ë²„í¼ëŠ” ë‹¤ë¥¸ ìš©ë„ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì§€
        self.manual_info_buffer = deque(maxlen=args.num_envs * 2)
        
    def _get_reward_object(self, env):
        """í™˜ê²½ì—ì„œ ì ì ˆí•œ ë³´ìƒ ê°ì²´ ì°¾ê¸°"""
        if hasattr(env, 'bipedal_reward'):
            return env.bipedal_reward
        elif hasattr(env, 'standing_reward'):
            return env.standing_reward
        elif hasattr(env, 'env'):
            if hasattr(env.env, 'bipedal_reward'):
                return env.env.bipedal_reward
            elif hasattr(env.env, 'standing_reward'):
                return env.env.standing_reward
        return None
        
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ"""
        
        dones = self.locals.get("dones", [])
        infos = self.locals.get("infos", [])

        # âš ï¸ [ìˆ˜ì •] ì¢…ë£Œ ì›ì¸ ì§‘ê³„ë¥¼ ìœ„í•´ ë²„í¼ë¥¼ ì±„ìš°ëŠ” ë¡œì§ì€ ìœ ì§€í•˜ë˜,
        # ì‚¬ìš©ì²˜ê°€ ì—†ì–´ì¡Œìœ¼ë¯€ë¡œ í–¥í›„ ë‹¤ë¥¸ í†µê³„ì— í™œìš©ë  ìˆ˜ ìˆìŒ.
        for i, done in enumerate(dones):
            if done:
                self.manual_info_buffer.append(copy.deepcopy(infos[i]))

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¡œì§
        if (self.num_timesteps - self.last_checkpoint >= self.args.checkpoint_interval):
            self._save_checkpoint()
            self.last_checkpoint = self.num_timesteps
            
        return True
    
    def _on_rollout_end(self) -> bool:
        """ë¡¤ì•„ì›ƒ ì¢…ë£Œ ì‹œ í†µê³„ ì¶œë ¥"""
        
        # âš ï¸ [ì œê±°] ì „ì—­ ì¢…ë£Œ ì›ì¸ ì§‘ê³„ ë¡œì§ ì œê±°
        
        # ë²„í¼ëŠ” ë¹„ì›€
        self.manual_info_buffer.clear()
        
        # ê¸°ì¡´ ì„±ëŠ¥ í‰ê°€ ë¡œì§
        if len(self.locals.get('episode_rewards', [])) > 0:
            recent_rewards = self.locals['episode_rewards'][-10:]
            mean_reward = np.mean(recent_rewards)
            self.episode_rewards.extend(recent_rewards)
            
            if mean_reward > self.best_reward:
                self.best_reward = mean_reward
                self.no_improvement_steps = 0
                self._save_best_model()
            else:
                self.no_improvement_steps += self.args.n_steps * self.args.num_envs
        
        # ê¸°íƒ€ í†µê³„ ìˆ˜ì§‘ ë° ì¶œë ¥
        self._log_upside_down_statistics()
        
        # âš ï¸ [ì œê±°] ì¢…ë£Œ ì›ì¸ í†µê³„ ì¶œë ¥ í•¨ìˆ˜ í˜¸ì¶œ ì œê±°
        
        # ì¡°ê¸° ì •ì§€ í™•ì¸
        if (self.args.early_stopping and 
            self.no_improvement_steps > self.patience):
            print(f"\nğŸ›‘ ì¡°ê¸° ì •ì§€: {self.patience:,} ìŠ¤í… ë™ì•ˆ ê°œì„  ì—†ìŒ")
            return False
            
        return True

    # âš ï¸ [ì œê±°] _log_termination_statistics ë©”ì„œë“œ ì „ì²´ ì œê±°

    def _log_upside_down_statistics(self):
        """í†µê³„ ë¡œê¹… - í™˜ê²½ë³„ ë³´ìƒ ê°ì²´ í˜¸í™˜"""
        try:
            # í™˜ê²½ì—ì„œ ì¹´ìš´íŠ¸ ìˆ˜ì§‘
            upside_down_counts = []
            
            # ëª¨ë“  ë³‘ë ¬ í™˜ê²½ì—ì„œ í†µê³„ ìˆ˜ì§‘
            if hasattr(self.training_env, 'envs'):
                for env in self.training_env.envs:
                    try:
                        reward_obj = self._get_reward_object(env)
                        if reward_obj:
                            count = getattr(reward_obj, 'upside_down_count', 0)
                            upside_down_counts.append(count)
                    except:
                        pass
            
            # í†µê³„ ê³„ì‚°
            if upside_down_counts:
                total_upside_down = sum(upside_down_counts)
                new_attempts = total_upside_down - self.last_upside_down_count
                avg_per_env = total_upside_down / len(upside_down_counts)
                
                # PPO ë¡œê·¸ì™€ í•¨ê»˜ ì¶œë ¥ë  ì¶”ê°€ ì •ë³´
                print(f"ğŸš¨ ë¬¼êµ¬ë‚˜ë¬´ì„œê¸° í†µê³„:")
                print(f"   ì´ ì‹œë„ íšŸìˆ˜: {total_upside_down}íšŒ")
                print(f"   ì´ë²ˆ ë¡¤ì•„ì›ƒ ìƒˆë¡œìš´ ì‹œë„: {new_attempts}íšŒ")
                print(f"   í™˜ê²½ë‹¹ í‰ê· : {avg_per_env:.1f}íšŒ")
                
                # TensorBoardì—ë„ ë¡œê¹…
                if hasattr(self.logger, 'record'):
                    self.logger.record("custom/total_upside_down_attempts", total_upside_down)
                    self.logger.record("custom/new_upside_down_attempts", new_attempts)
                    self.logger.record("custom/avg_upside_down_per_env", avg_per_env)
                
                self.last_upside_down_count = total_upside_down
                
        except Exception as e:
            print(f"âš ï¸ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_dir = Path("checkpoints") / f"{self.args.task}_training"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_path = checkpoint_dir / f"checkpoint_{self.num_timesteps}.zip"
        self.model.save(checkpoint_path)
        
        # í†µê³„ë„ ë©”íƒ€ë°ì´í„°ì— í¬í•¨
        try:
            reward_obj = self._get_reward_object(self.eval_env)
            upside_down_count = getattr(reward_obj, 'upside_down_count', 0) if reward_obj else 0
        except:
            upside_down_count = 0
        
        metadata = {
            'timesteps': self.num_timesteps,
            'best_reward': self.best_reward,
            'upside_down_attempts': upside_down_count,
            'args': vars(self.args)
        }
        
        import json
        with open(checkpoint_dir / f"metadata_{self.num_timesteps}.json", 'w') as f:
            json.dump(metadata, f, indent=2)
            
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path} (ë¬¼êµ¬ë‚˜ë¬´ ì‹œë„: {upside_down_count}íšŒ)")
    
    def _save_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        best_dir = Path("models") / "best"
        best_dir.mkdir(parents=True, exist_ok=True)
        
        # í†µê³„ í¬í•¨
        try:
            reward_obj = self._get_reward_object(self.eval_env)
            upside_down_count = getattr(reward_obj, 'upside_down_count', 0) if reward_obj else 0
            upside_down_info = f" (ë¬¼êµ¬ë‚˜ë¬´: {upside_down_count}íšŒ)"
        except:
            upside_down_info = ""
        
        best_path = best_dir / f"{self.args.task}_best_{self.num_timesteps}.zip"
        self.model.save(best_path)
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path} (ë³´ìƒ: {self.best_reward:.2f}){upside_down_info}")

def train_with_optimized_parameters(args):  
    """2ì¡± ë³´í–‰ ìµœì í™”ëœ í›ˆë ¨ - ê´€ì°° ê³µê°„ í˜¸í™˜ì„± ìˆ˜ì •"""
    print(f"\nğŸš€ 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì‹œì‘! (task={args.task})")
    print(f"ğŸ“Š ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(f"  - í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"  - ë¡¤ì•„ì›ƒ ìŠ¤í…: {args.n_steps}")
    print(f"  - í´ë¦½ ë²”ìœ„: {args.clip_range}")
    print(f"  - ì—”íŠ¸ë¡œí”¼: {args.entropy_coef}")
    print(f"  - ëª©í‘œ ì†ë„: {args.target_vel} m/s")
    print(f"  - ì•ˆì •ì„± ê°€ì¤‘ì¹˜: {args.stability_weight}")
    print(f"  - ë†’ì´ í—ˆìš©ì˜¤ì°¨: {args.height_tolerance}")
    print(f"  - ë³‘ë ¬ í™˜ê²½ ìˆ˜: {args.num_envs}")
    print(f"  - ì´ í›ˆë ¨ ìŠ¤í…: {args.total_timesteps:,}")
    print(f"  - ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: {'ì‚¬ìš©' if args.use_curriculum else 'ë¯¸ì‚¬ìš©'}")
    print(f"  - ì¡°ê¸° ì •ì§€: {'ì‚¬ìš©' if args.early_stopping else 'ë¯¸ì‚¬ìš©'}")
    
    # âœ… ëœë¤ì„± ê°•ë„ ì„¤ì • ì¶”ê°€
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ì—ì„œ ëœë¤ì„± ê°•ë„ ê°€ì ¸ì˜¤ê¸°
    randomness_intensity = args.randomness_intensity
    RobotPhysicsUtils.set_randomness_intensity(randomness_intensity)
    print(f"ğŸ›ï¸ ëœë¤ì„± ê°•ë„ ì„¤ì •: {randomness_intensity}")
    
    # í™˜ê²½ì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨
    env_kwargs = {
        'randomize_physics': True,
    }
    
    # í™˜ê²½ ì„ íƒ
    if args.task == "standing":
        if args.use_curriculum:
            env_class = GradualStandingEnv
            print("ğŸ“š ì ì§„ì  ì»¤ë¦¬í˜ëŸ¼ í™˜ê²½ ì‚¬ìš©")
        else:
            env_class = BipedalWalkingEnv
            print("ğŸ¯ 2ì¡± ë³´í–‰ í™˜ê²½ ì‚¬ìš©")
    else:
        env_class = Go1MujocoEnv
        print("ğŸ• ê¸°ë³¸ 4ì¡± ë³´í–‰ í™˜ê²½ ì‚¬ìš©")
    
    # ì‚¬ì „í›ˆë ¨ ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸
    use_pretrained = False
    compatible_env_kwargs = env_kwargs.copy()
    
    if args.pretrained_model:
        print(f"\nğŸ” ì‚¬ì „í›ˆë ¨ ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸ ì¤‘...")
        
        # ëª¨ë¸ ê²½ë¡œ í™•ì¸
        pretrained_model_path = args.pretrained_model
        if pretrained_model_path == "latest":
            models = glob.glob(f"./models/{args.task}*.zip")
            if models:
                pretrained_model_path = list(sorted(models))[-1]
            else:
                print("âŒ 'latest' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                pretrained_model_path = None
        
        if pretrained_model_path and os.path.exists(pretrained_model_path):
            # ì„ì‹œ í™˜ê²½ ìƒì„±í•´ì„œ ê´€ì°° ê³µê°„ í™•ì¸
            temp_env = env_class(**env_kwargs)
            is_compatible = check_observation_compatibility(pretrained_model_path, temp_env)
            temp_env.close()
            
            if is_compatible:
                use_pretrained = True
                print("âœ… ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
            elif args.ignore_pretrained_obs_mismatch:
                print("âš ï¸ ê´€ì°° ê³µê°„ ë¶ˆì¼ì¹˜ë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆ ëª¨ë¸ ìƒì„±")
                use_pretrained = False
            else:
                print("âŒ ê´€ì°° ê³µê°„ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")
                print("  í•´ê²°ì±…:")
                print("  1. --ignore_pretrained_obs_mismatch í”Œë˜ê·¸ ì¶”ê°€")
                print("  2. ë™ì¼í•œ í™˜ê²½ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©")
                print("  3. ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ì—†ì´ ìƒˆë¡œ ì‹œì‘")
                
                # í˜¸í™˜ ëª¨ë“œë¡œ í™˜ê²½ ì„¤ì • ì‹œë„
                print("  4. í˜¸í™˜ ëª¨ë“œë¡œ í™˜ê²½ ì„¤ì • ì‹œë„ ì¤‘...")
                try:
                    compatible_env_kwargs['use_base_observation'] = True
                    temp_env_compat = env_class(**compatible_env_kwargs)
                    is_compatible_retry = check_observation_compatibility(pretrained_model_path, temp_env_compat)
                    temp_env_compat.close()
                    
                    if is_compatible_retry:
                        print("âœ… í˜¸í™˜ ëª¨ë“œë¡œ ì„¤ì • ì„±ê³µ!")
                        use_pretrained = True
                        env_kwargs = compatible_env_kwargs  # í˜¸í™˜ ëª¨ë“œ ì ìš©
                    else:
                        print("âŒ í˜¸í™˜ ëª¨ë“œë¡œë„ í•´ê²°ë˜ì§€ ì•ŠìŒ")
                        # ì‚¬ìš©ì ì„ íƒ ëŒ€ê¸°
                        choice = input("\nìƒˆ ëª¨ë¸ë¡œ í›ˆë ¨ì„ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
                        if choice != 'y':
                            print("í›ˆë ¨ ì¤‘ë‹¨")
                            return
                        use_pretrained = False
                except Exception as e:
                    print(f"âš ï¸ í˜¸í™˜ ëª¨ë“œ ì„¤ì • ì‹¤íŒ¨: {e}")
                    choice = input("\nìƒˆ ëª¨ë¸ë¡œ í›ˆë ¨ì„ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
                    if choice != 'y':
                        print("í›ˆë ¨ ì¤‘ë‹¨")
                        return
                    use_pretrained = False
        else:
            print(f"âŒ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pretrained_model_path}")
            use_pretrained = False
    
    # í•™ìŠµìš© í™˜ê²½ (ë³‘ë ¬í™”) - í˜¸í™˜ì„± ì ìš©ëœ kwargs ì‚¬ìš©
    print(f"\nğŸ­ {args.num_envs}ê°œ ë³‘ë ¬ í™˜ê²½ ìƒì„± ì¤‘...")
    vec_env = make_vec_env(
        env_class, 
        n_envs=args.num_envs, 
        vec_env_cls=SubprocVecEnv,
        env_kwargs=env_kwargs  # í˜¸í™˜ì„± ì„¤ì •ì´ ì ìš©ëœ kwargs
    )
    
    # í‰ê°€ìš© í™˜ê²½ - í˜¸í™˜ì„± ì ìš©ëœ kwargs ì‚¬ìš©
    print("ğŸ“Š í‰ê°€ í™˜ê²½ ìƒì„± ì¤‘...")
    eval_env = env_class(render_mode="rgb_array", **env_kwargs)
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EnhancedVisualCallback(
            eval_env,
            eval_interval_minutes=args.visual_interval,
            n_eval_episodes=3,
            show_duration_seconds=args.show_duration,
            save_videos=args.save_videos,
            use_curriculum=args.use_curriculum
        ),
        StandingTrainingCallback(args, eval_env)
    ]
    
    # ë¹„ë””ì˜¤ ë…¹í™” ì½œë°±
    if args.video_interval > 0:
        record_env = DummyVecEnv([lambda: env_class(render_mode="rgb_array", **env_kwargs)])
        callbacks.append(
            VideoRecordingCallback(
                record_env,
                record_interval_timesteps=args.video_interval,
                video_folder=f"eval_videos_{args.task}",
                show_duration_seconds=args.show_duration
            )
        )
    
    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    tensorboard_log = f"logs/{args.task}_optimized_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if use_pretrained:
        print(f"ğŸ“‚ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ: {pretrained_model_path}")
        model = PPO.load(pretrained_model_path, env=vec_env)
        model.set_env(vec_env)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        if hasattr(model, 'learning_rate'):
            if args.use_curriculum:
                def lr_schedule(progress_remaining):
                    if progress_remaining > 0.9:
                        return args.learning_rate * 1.2
                    elif progress_remaining > 0.7:
                        return args.learning_rate
                    elif progress_remaining > 0.3:
                        return args.learning_rate * 0.5
                    else:
                        return args.learning_rate * 0.2
                model.learning_rate = lr_schedule
            else:
                model.learning_rate = args.learning_rate
        
        if hasattr(model, 'clip_range'):
            def clip_range_func(progress_remaining):
                return args.clip_range
            model.clip_range = clip_range_func
        
        print(f"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
    else:
        print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_optimized_ppo_model(vec_env, args, tensorboard_log)
    
    # training_time ì´ˆê¸°í™”
    training_time = 0.0
    
    # í•™ìŠµ ì‹œì‘
    try:
        print(f"\nğŸ¯ 2ì¡± ë³´í–‰ ìµœì í™” í•™ìŠµ ì‹œì‘...")
        print(f"ğŸ“Š TensorBoard ë¡œê·¸: {tensorboard_log}")
        print("ğŸ’¡ TensorBoard ì‹¤í–‰: tensorboard --logdir=logs")
        print("ğŸ“ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ ë³„ë„ í„°ë¯¸ë„ì—ì„œ TensorBoardë¥¼ ì‹¤í–‰í•˜ì„¸ìš”\n")
        
        start_time = time.time()
        
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=False if use_pretrained else True
        )
        
        training_time = time.time() - start_time
        print(f"\nâ±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        
    except KeyboardInterrupt:
        training_time = time.time() - start_time if 'start_time' in locals() else 0.0
        print(f"\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ - í˜„ì¬ ìƒíƒœ ì €ì¥ ì¤‘... (ì§„í–‰ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„)")
    except Exception as e:
        training_time = time.time() - start_time if 'start_time' in locals() else 0.0
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"â±ï¸ ì§„í–‰ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        import traceback
        traceback.print_exc()
    
    # ìµœì¢… ì €ì¥ ë° ë¶„ì„
    print("\nğŸ’¾ ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ë³´ê³ ì„œ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"training_reports_{args.task}_optimized_{timestamp}"
    os.makedirs(report_path, exist_ok=True)
    
    if len(callbacks) > 0 and hasattr(callbacks[0], 'save_progress_report'):
        callbacks[0].save_progress_report(report_path)
        if hasattr(callbacks[0], 'save_detailed_analysis'):
            callbacks[0].save_detailed_analysis(report_path)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    model_path = f"models/{args.task}_optimized_final_{timestamp}.zip"
    Path("models").mkdir(exist_ok=True)
    model.save(model_path)
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")
    
    # ì„¤ì • ì €ì¥
    config_path = os.path.join(report_path, "optimized_training_config.txt")
    with open(config_path, 'w') as f:
        f.write("=== 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì„¤ì • ===\n\n")
        f.write(f"Task: {args.task}\n")
        f.write(f"Total timesteps: {args.total_timesteps:,}\n")
        f.write(f"Training time: {training_time/3600:.2f} hours\n")
        f.write(f"Used pretrained model: {use_pretrained}\n")
        f.write(f"Randomness intensity: {randomness_intensity}\n")  # âœ… ëœë¤ì„± ê°•ë„ ê¸°ë¡
        if use_pretrained:
            f.write(f"Pretrained model path: {pretrained_model_path}\n")
        f.write(f"Environment observation mode: {'Base(45dim)' if env_kwargs.get('use_base_observation', False) else 'Extended(56dim)'}\n")
        f.write("\n")
        
        f.write("=== í™˜ê²½ ì„¤ì • ===\n")
        f.write(f"Environment class: {env_class.__name__}\n")
        f.write(f"Num environments: {args.num_envs}\n")
        f.write(f"Curriculum learning: {args.use_curriculum}\n")
        f.write(f"Target velocity: {args.target_vel} m/s\n")
        f.write(f"Stability weight: {args.stability_weight}\n")
        f.write(f"Height tolerance: {args.height_tolerance}\n")
        f.write(f"Use base observation: {env_kwargs.get('use_base_observation', False)}\n\n")
        
        f.write("=== PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ===\n")
        f.write(f"Learning rate: {args.learning_rate}\n")
        f.write(f"Batch size: {args.batch_size}\n")
        f.write(f"N steps: {args.n_steps}\n")
        f.write(f"Clip range: {args.clip_range}\n")
        f.write(f"Entropy coefficient: {args.entropy_coef}\n")
        f.write(f"Early stopping: {args.early_stopping}\n\n")
        
        f.write("=== íŒŒì¼ ê²½ë¡œ ===\n")
        f.write(f"Final model: {model_path}\n")
        f.write(f"TensorBoard logs: {tensorboard_log}\n")
        f.write(f"Training reports: {report_path}\n")
        
        if use_pretrained:
            f.write(f"Original pretrained model: {args.pretrained_model}\n")
            f.write(f"Observation compatibility: {'Compatible' if use_pretrained else 'Incompatible - created new model'}\n")
   
    # ìµœì¢… í‰ê°€
    print(f"\nğŸ§ª ìµœì¢… ëª¨ë¸ í‰ê°€ ì¤‘...")
    try:
        final_rewards = []
        final_successes = []
        
        for i in range(5):  # 5íšŒ í‰ê°€
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            
            for _ in range(1000):  # ìµœëŒ€ 1000 ìŠ¤í…
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if terminated or truncated:
                    break
            
            final_rewards.append(episode_reward)
            # í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ ì„±ê³µ í‚¤ ì‚¬ìš©
            if hasattr(eval_env, 'bipedal_reward'):
                success_key = 'bipedal_success'
            else:
                success_key = 'standing_success'
            final_successes.append(info.get(success_key, False))
            print(f"  í‰ê°€ {i+1}: ë³´ìƒ={episode_reward:.2f}, ê¸¸ì´={episode_length}, ì„±ê³µ={info.get(success_key, False)}")
        
        mean_reward = np.mean(final_rewards)
        success_rate = np.mean(final_successes)
        
        print(f"\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
        print(f"  í‰ê·  ë³´ìƒ: {mean_reward:.2f} Â± {np.std(final_rewards):.2f}")
        print(f"  ì„±ê³µë¥ : {success_rate:.1%}")
        
        # ê²°ê³¼ë¥¼ config íŒŒì¼ì— ì¶”ê°€
        with open(config_path, 'a') as f:
            f.write(f"\n=== ìµœì¢… í‰ê°€ ê²°ê³¼ ===\n")
            f.write(f"Mean reward: {mean_reward:.2f} Â± {np.std(final_rewards):.2f}\n")
            f.write(f"Success rate: {success_rate:.1%}\n")
            
    except Exception as e:
        print(f"âš ï¸ ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {e}")
    
    # ì •ë¦¬
    eval_env.close()
    vec_env.close()
    if 'record_env' in locals():
        record_env.close()
    
    print(f"\nğŸ‰ 2ì¡± ë³´í–‰ ìµœì í™” í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {report_path}")
    print(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. TensorBoard í™•ì¸: tensorboard --logdir=logs")
    print(f"   2. ë³´ê³ ì„œ í™•ì¸: {report_path}")
    print(f"   3. ëª¨ë¸ í…ŒìŠ¤íŠ¸: python test_model.py --model {model_path}")
    print(f"   4. ë¹„ë””ì˜¤ í™•ì¸: eval_videos_{args.task}/")
    
    if args.use_curriculum:
        print(f"   5. ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ìƒí™©ì€ TensorBoardì—ì„œ í™•ì¸í•˜ì„¸ìš”")
    
    if not use_pretrained and args.pretrained_model:
        print(f"\nğŸ’¡ ì°¸ê³ : ê´€ì°° ê³µê°„ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ìƒˆ ëª¨ë¸ë¡œ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"   í˜¸í™˜ ê°€ëŠ¥í•œ ì‚¬ì „í›ˆë ¨ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´:")
        print(f"   1. ê°™ì€ í™˜ê²½ í´ë˜ìŠ¤ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©")
        print(f"   2. ë˜ëŠ” --ignore_pretrained_obs_mismatch í”Œë˜ê·¸ ì‚¬ìš©")
        print(f"   3. ë˜ëŠ” í™˜ê²½ì— use_base_observation=True ì„¤ì •")


if __name__ == "__main__":
    args = parse_arguments()
    train_with_optimized_parameters(args)