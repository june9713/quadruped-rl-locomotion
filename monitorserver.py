import threading
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO  # PPO ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„í¬íŠ¸
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv
import gymnasium as gym
from collections import deque
import os
import time
import imageio.v2 as imageio
import pandas as pd
import datetime
from go1_mujoco_env import Go1MujocoEnv
import copy
import subprocess
import sys
import json
from matplotlib.gridspec import GridSpec


def check_and_install_moviepy():
    """moviepy ì„¤ì¹˜ í™•ì¸ ë° ìë™ ì„¤ì¹˜"""
    try:
        import moviepy
        print("âœ… moviepyê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return True
    except ImportError:
        print("ğŸ“¦ moviepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "moviepy"])
            print("âœ… moviepy ì„¤ì¹˜ ì™„ë£Œ!")
            return True
        except subprocess.CalledProcessError:
            print("âŒ moviepy ì„¤ì¹˜ ì‹¤íŒ¨. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install moviepy")
            return False


class VisualTrainingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ê°„ì— ì‹œê°ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì½œë°±"""
    
    def __init__(
        self,
        eval_env: gym.Env,
        eval_interval_minutes: int = 10,
        n_eval_episodes: int = 3,
        show_duration_seconds: int = 30,
        save_videos: bool = True,
        verbose: int = 1,
    ):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_interval_seconds = eval_interval_minutes * 60
        self.n_eval_episodes = n_eval_episodes
        self.show_duration_seconds = show_duration_seconds
        self.save_videos = save_videos
        self.step_zero = True
        
        self.last_eval_time = time.time()
        self.eval_count = 0
        self.performance_history = deque(maxlen=50)
        
        # ì‹¤ì‹œê°„ í”Œë¡¯ì„ ìœ„í•œ ì„¤ì •
        plt.ion()
        self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
        self.fig.suptitle('ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©', fontsize=16)
        
        # ì„œë¸Œí”Œë¡¯ ì„¤ì •
        self.reward_ax = self.axes[0, 0]
        self.episode_length_ax = self.axes[0, 1] 
        self.success_rate_ax = self.axes[1, 0]
        self.learning_curve_ax = self.axes[1, 1]
        
        # ë°ì´í„° ì €ì¥ìš©
        self.rewards_history = []
        self.lengths_history = []
        self.success_rates = []
        self.timesteps_history = []
        
    def _on_step(self) -> bool:
        current_time = time.time()
        
        # ì§€ì •ëœ ì‹œê°„ ê°„ê²©ë§ˆë‹¤ í‰ê°€ ë° ì‹œê°í™”
        if self.step_zero or (current_time - self.last_eval_time >= self.eval_interval_seconds):
            self.step_zero = False
            self._evaluate_and_visualize()
            self.last_eval_time = current_time
            
        return True
    
    def _evaluate_and_visualize(self):
        """ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™” + MP4 ì €ì¥"""
        self.eval_count += 1

        print(f"\n{'='*60}")
        print(f"ğŸ“Š í‰ê°€ #{self.eval_count} (Timestep: {self.num_timesteps:,})")
        print(f"â° ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {self.show_duration_seconds}ì´ˆ")
        print(f"{'='*60}")

        episode_rewards = []
        episode_lengths = []
        success_count = 0
        self.n_eval_episodes  =2
        for episode in range(self.n_eval_episodes):
            print(f"  ğŸ® ì—í”¼ì†Œë“œ {episode + 1}/{self.n_eval_episodes} ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")

            obs, _ = self.eval_env.reset()
            episode_reward = 0.0
            episode_length = 0
            frames = []

            start_time = time.time()

            while True:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)

                episode_reward += float(reward)
                episode_length += 1

                # í”„ë ˆì„ ìº¡ì²˜
                frame = None
                try:
                    frame = self.eval_env.render()
                    if isinstance(frame, list):
                        frame = frame[0]
                except Exception:
                    try:
                        frame = self.eval_env.render(mode="rgb_array")
                    except Exception:
                        frame = None

                if frame is not None:
                    frames.append(frame)

                time.sleep(0.02)

                if time.time() - start_time >= self.show_duration_seconds:
                    print(f"    â° ì‹œê°„ ì œí•œ ({self.show_duration_seconds}ì´ˆ) ë„ë‹¬")
                    break

                if terminated or truncated:
                    if info.get('standing_success', False):
                        success_count += 1
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            print(f"    ğŸ“ˆ ë³´ìƒ: {episode_reward:.2f}, ê¸¸ì´: {episode_length}")

        # ê²°ê³¼ ì§‘ê³„
        mean_r, std_r = np.mean(episode_rewards), np.std(episode_rewards)
        mean_l, std_l = np.mean(episode_lengths), np.std(episode_lengths)
        success_rate = success_count / self.n_eval_episodes

        self.rewards_history.append(mean_r)
        self.lengths_history.append(mean_l)
        self.success_rates.append(success_rate)
        self.timesteps_history.append(self.num_timesteps)

        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"  í‰ê·  ë³´ìƒ: {mean_r:.2f} Â± {std_r:.2f}")
        print(f"  í‰ê·  ê¸¸ì´: {mean_l:.1f} Â± {std_l:.1f}")
        print(f"  ì„±ê³µë¥ : {success_rate:.1%} ({success_count}/{self.n_eval_episodes})")
        print(f"{'='*60}\n")

        self._update_plots()
   
    def _update_plots(self):
        """ì‹¤ì‹œê°„ í”Œë¡¯ ì—…ë°ì´íŠ¸"""
        
        # 1. ë³´ìƒ ì¶”ì´
        self.reward_ax.clear()
        if self.rewards_history:
            self.reward_ax.plot(self.timesteps_history, self.rewards_history, 'b-o', linewidth=2)
            self.reward_ax.set_title('í‰ê·  ë³´ìƒ ì¶”ì´')
            self.reward_ax.set_xlabel('Timesteps')
            self.reward_ax.set_ylabel('í‰ê·  ë³´ìƒ')
            self.reward_ax.grid(True, alpha=0.3)
        
        # 2. ì—í”¼ì†Œë“œ ê¸¸ì´ ì¶”ì´
        self.episode_length_ax.clear()
        if self.lengths_history:
            self.episode_length_ax.plot(self.timesteps_history, self.lengths_history, 'g-o', linewidth=2)
            self.episode_length_ax.set_title('í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´')
            self.episode_length_ax.set_xlabel('Timesteps')
            self.episode_length_ax.set_ylabel('í‰ê·  ê¸¸ì´')
            self.episode_length_ax.grid(True, alpha=0.3)
        
        # 3. ì„±ê³µë¥  ì¶”ì´
        self.success_rate_ax.clear()
        if self.success_rates:
            self.success_rate_ax.plot(self.timesteps_history, 
                                      [r*100 for r in self.success_rates], 'r-o', linewidth=2)
            self.success_rate_ax.set_title('ì„±ê³µë¥  ì¶”ì´')
            self.success_rate_ax.set_xlabel('Timesteps')
            self.success_rate_ax.set_ylabel('ì„±ê³µë¥  (%)')
            self.success_rate_ax.grid(True, alpha=0.3)
            self.success_rate_ax.set_ylim(0, 100)
        
        # 4. ìµœê·¼ ì„±ëŠ¥ (ë°•ìŠ¤í”Œë¡¯ ë˜ëŠ” íˆìŠ¤í† ê·¸ë¨)
        self.learning_curve_ax.clear()
        if len(self.rewards_history) > 1:
            recent_rewards = self.rewards_history[-10:]
            self.learning_curve_ax.hist(recent_rewards, bins=max(3, len(recent_rewards)//2), 
                                        alpha=0.7, color='purple')
            self.learning_curve_ax.set_title('ìµœê·¼ ë³´ìƒ ë¶„í¬')
            self.learning_curve_ax.set_xlabel('ë³´ìƒ')
            self.learning_curve_ax.set_ylabel('ë¹ˆë„')
            self.learning_curve_ax.axvline(np.mean(recent_rewards), color='red', 
                                           linestyle='--', label=f'í‰ê· : {np.mean(recent_rewards):.2f}')
            self.learning_curve_ax.legend()
        
        plt.tight_layout()
        plt.pause(0.1)
   
    def save_progress_report(self, save_path: str):
        """ì§„í–‰ ìƒí™© ë³´ê³ ì„œ ì €ì¥"""
        if not self.rewards_history:
            return
            
        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
        plt.figure(figsize=(15, 10))
        
        # 2x2 ì„œë¸Œí”Œë¡¯
        plt.subplot(2, 2, 1)
        plt.plot(self.timesteps_history, self.rewards_history, 'b-o', linewidth=2)
        plt.title('í•™ìŠµ ì§„í–‰: í‰ê·  ë³´ìƒ', fontsize=14)
        plt.xlabel('Timesteps')
        plt.ylabel('í‰ê·  ë³´ìƒ')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 2)
        plt.plot(self.timesteps_history, self.lengths_history, 'g-o', linewidth=2)
        plt.title('í•™ìŠµ ì§„í–‰: í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´', fontsize=14)
        plt.xlabel('Timesteps')
        plt.ylabel('í‰ê·  ê¸¸ì´')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 3)
        plt.plot(self.timesteps_history, [r*100 for r in self.success_rates], 'r-o', linewidth=2)
        plt.title('í•™ìŠµ ì§„í–‰: ì„±ê³µë¥ ', fontsize=14)
        plt.xlabel('Timesteps')
        plt.ylabel('ì„±ê³µë¥  (%)')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 100)
        
        plt.subplot(2, 2, 4)
        if len(self.rewards_history) > 5:
            # í•™ìŠµ ê³¡ì„ ì˜ ê¸°ìš¸ê¸° (ê°œì„  ì†ë„)
            improvement_rate = np.diff(self.rewards_history)
            plt.plot(self.timesteps_history[1:], improvement_rate, 'purple', linewidth=2)
            plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
            plt.title('í•™ìŠµ ê°œì„  ì†ë„', fontsize=14)
            plt.xlabel('Timesteps')
            plt.ylabel('ë³´ìƒ ê°œì„ ëŸ‰')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_path}/training_progress.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # CSVë¡œ ë°ì´í„° ì €ì¥
        df = pd.DataFrame({
            'timesteps': self.timesteps_history,
            'mean_reward': self.rewards_history,
            'mean_length': self.lengths_history,
            'success_rate': self.success_rates
        })
        df.to_csv(f"{save_path}/training_data.csv", index=False)
        
        print(f"ğŸ“Š ì§„í–‰ ìƒí™© ë³´ê³ ì„œ ì €ì¥: {save_path}")


class EnhancedVisualCallback(VisualTrainingCallback):
    """ê°œì„ ëœ ì‹œê°í™” ì½œë°± - ë” ë§ì€ ë¶„ì„ ê¸°ëŠ¥"""
    
    def __init__(self, *args, use_curriculum=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.use_curriculum = use_curriculum
        
        # ì¶”ê°€ ì¶”ì  ë°ì´í„°
        self.reward_components_history = []
        self.curriculum_stages = []
        self.stability_metrics = []
        self.failure_reasons = []
        
        # --- ë ˆì´ì•„ì›ƒ ìˆ˜ì • ---
        # ê°œì„ ëœ í”Œë¡¯ ì„¤ì • - í‰ê·  ë³´ìƒ ê·¸ë˜í”„ë¥¼ ìµœìƒë‹¨ì— ë°°ì¹˜
        plt.ioff()
        self.fig = plt.figure(figsize=(16, 18))  # ì„¸ë¡œ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ ê·¸ë˜í”„ ê³µê°„ í™•ë³´
        gs = GridSpec(4, 2, figure=self.fig, hspace=0.4, wspace=0.3)

        # í–‰ 0: í‰ê·  ë³´ìƒ (ì „ì²´ ë„ˆë¹„)
        self.reward_ax = self.fig.add_subplot(gs[0, :])

        # í–‰ 1: ë³´ìƒ ì»´í¬ë„ŒíŠ¸ì™€ ì„±ê³µë¥ 
        self.components_ax = self.fig.add_subplot(gs[1, 0])
        self.success_ax = self.fig.add_subplot(gs[1, 1])

        # í–‰ 2: ì•ˆì •ì„± ë©”íŠ¸ë¦­ê³¼ ì‹¤íŒ¨ ì›ì¸
        self.stability_ax = self.fig.add_subplot(gs[2, 0])
        self.failure_ax = self.fig.add_subplot(gs[2, 1])

        # í–‰ 3: ë³´ìƒ ì»´í¬ë„ŒíŠ¸ íˆíŠ¸ë§µ (ì „ì²´ ë„ˆë¹„)
        self.heatmap_ax = self.fig.add_subplot(gs[3, :])
        
        # ì›ë˜ ìˆë˜ ë‘ ê°œì˜ ì¶•ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        self.learning_curve_ax = None
        self.episode_length_ax = None
        
        plt.ion()
   
    def _evaluate_and_visualize(self):
        """ê°œì„ ëœ í‰ê°€ ë° ì‹œê°í™”"""
        self.eval_count += 1
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ê³ ê¸‰ í‰ê°€ #{self.eval_count} (Timestep: {self.num_timesteps:,})")
        print(f"{'='*70}")
        
        # ê¸°ë³¸ í‰ê°€ ë°ì´í„°
        episode_rewards = []
        episode_lengths = []
        episode_components = []
        episode_stability = []
        episode_failures = []
        success_count = 0
        self.n_eval_episodes = 2
        for episode in range(self.n_eval_episodes):
            print(f"\nğŸ® ì—í”¼ì†Œë“œ {episode + 1}/{self.n_eval_episodes}")
            
            obs, _ = self.eval_env.reset()
            episode_reward = 0.0
            episode_length = 0
            
            # ì—í”¼ì†Œë“œë³„ ìƒì„¸ ì¶”ì 
            reward_components = {}
            stability_metrics = []
            frames = []
            
            start_time = time.time()
            
            while True:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)
                
                episode_reward += float(reward)
                episode_length += 1
                
                # ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ì¶”ì 
                if 'upright' in info:
                    for key in ['upright', 'height', 'feet', 'forward_vel', 
                               'lateral_vel', 'cop_stab', 'zmp_stab']:
                        if key in info:
                            if key not in reward_components:
                                reward_components[key] = []
                            reward_components[key].append(info.get(key, 0))
                
                # ì•ˆì •ì„± ë©”íŠ¸ë¦­
                if 'stab_ang' in info:
                    stability_metrics.append({
                        'angular_stability': info.get('stab_ang', 0),
                        'cop_stability': info.get('cop_stab', 0),
                        'zmp_stability': info.get('zmp_stab', 0)
                    })
                
                # í”„ë ˆì„ ìº¡ì²˜
                if self.save_videos:
                    try:
                        frame = self.eval_env.render()
                        if frame is not None:
                            frames.append(frame)
                    except:
                        pass
                
                time.sleep(0.01)
                
                # ì¢…ë£Œ ì¡°ê±´
                if time.time() - start_time >= self.show_duration_seconds:
                    break
                    
                if terminated or truncated:
                    # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ì„
                    if info.get('standing_success', False):
                        success_count += 1
                    else:
                        # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
                        failure_reason = self._analyze_failure(info, obs)
                        episode_failures.append(failure_reason)
                    break
            
            # ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # í‰ê·  ì»´í¬ë„ŒíŠ¸ ê°’
            avg_components = {}
            for key, values in reward_components.items():
                if values:
                    avg_components[key] = np.mean(values)
            episode_components.append(avg_components)
            
            # í‰ê·  ì•ˆì •ì„±
            if stability_metrics:
                avg_stability = {
                    key: np.mean([m[key] for m in stability_metrics])
                    for key in stability_metrics[0].keys()
                }
                episode_stability.append(avg_stability)
            
            print(f"  ğŸ“ˆ ë³´ìƒ: {episode_reward:.2f}")
            print(f"  â±ï¸ ê¸¸ì´: {episode_length}")
            print(f"  ğŸ¯ ì£¼ìš” ì»´í¬ë„ŒíŠ¸: {', '.join([f'{k}:{v:.2f}' for k,v in avg_components.items()][:3])}")
            
            # ë¹„ë””ì˜¤ ì €ì¥
            if self.save_videos and frames:
                self._save_video(frames, episode, episode_reward)
        
        # ì „ì²´ í‰ê°€ ê²°ê³¼ ì €ì¥
        self._update_history(episode_rewards, episode_lengths, 
                            episode_components, episode_stability, 
                            episode_failures, success_count)
        
        # í”Œë¡¯ ì—…ë°ì´íŠ¸
        self._update_enhanced_plots()
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š í‰ê°€ ìš”ì•½:")
        print(f"  í‰ê·  ë³´ìƒ: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
        print(f"  ì„±ê³µë¥ : {success_count/self.n_eval_episodes:.1%}")
        if self.use_curriculum and hasattr(self.eval_env, 'standing_reward'):
            print(f"  ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„: {self.eval_env.standing_reward.curriculum_stage}")
        print(f"{'='*70}\n")
   
    def _analyze_failure(self, info, obs):
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        reasons = []
        
        if info.get('upright', 1) < 0.5:
            reasons.append('fall_forward' if obs[0] > 0 else 'fall_backward')
        if info.get('height', 1) < 0.3:
            reasons.append('too_low')
        if info.get('lateral_vel', 1) < 0.5:
            reasons.append('lateral_instability')
        if info.get('joint_limit', 0) < -0.5:
            reasons.append('joint_limit_violation')
        
        return reasons[0] if reasons else 'unknown'
   
    def _save_video(self, frames, episode, reward):
        """ë¹„ë””ì˜¤ ì €ì¥"""
        os.makedirs("eval_videos", exist_ok=True)
        filename = (f"eval_videos/enhanced_eval{self.eval_count}_ep{episode+1}_"
                   f"r{reward:.0f}_t{self.num_timesteps}.mp4")
        try:
            imageio.mimsave(filename, frames, fps=30)
        except:
            pass
   
    def _update_history(self, rewards, lengths, components, stability, failures, successes):
        """íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        self.rewards_history.append(np.mean(rewards))
        self.lengths_history.append(np.mean(lengths))
        self.success_rates.append(successes / self.n_eval_episodes)
        self.timesteps_history.append(self.num_timesteps)
        
        # í‰ê·  ì»´í¬ë„ŒíŠ¸
        avg_components = {}
        if components:
            keys = components[0].keys()
            for key in keys:
                values = [c.get(key, 0) for c in components]
                avg_components[key] = np.mean(values)
        self.reward_components_history.append(avg_components)
        
        # ì•ˆì •ì„± ë©”íŠ¸ë¦­
        if stability:
            avg_stability = {}
            keys = stability[0].keys()
            for key in keys:
                values = [s.get(key, 0) for s in stability]
                avg_stability[key] = np.mean(values)
            self.stability_metrics.append(avg_stability)
        
        # ì‹¤íŒ¨ ë¶„ì„
        failure_counts = {}
        for f in failures:
            failure_counts[f] = failure_counts.get(f, 0) + 1
        self.failure_reasons.append(failure_counts)
        
        # ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„
        if self.use_curriculum and hasattr(self.eval_env, 'standing_reward'):
            self.curriculum_stages.append(self.eval_env.standing_reward.curriculum_stage)
   
    def _update_enhanced_plots(self):
        """ê°œì„ ëœ í”Œë¡¯ ì—…ë°ì´íŠ¸"""
        plt.figure(self.fig.number)
        
        # 1. ì „ì²´ ë³´ìƒ ì¶”ì´
        self.reward_ax.clear()
        self.reward_ax.plot(self.timesteps_history, self.rewards_history, 'b-', linewidth=2)
        if len(self.rewards_history) > 10:
            # ì´ë™ í‰ê· 
            window = min(10, len(self.rewards_history))
            ma = np.convolve(self.rewards_history, np.ones(window)/window, mode='valid')
            ma_x = self.timesteps_history[window-1:]
            self.reward_ax.plot(ma_x, ma, 'r--', linewidth=2, label='ì´ë™í‰ê· ')
        self.reward_ax.set_title('í•™ìŠµ ì§„í–‰: í‰ê·  ë³´ìƒ', fontsize=14, weight='bold') # ì œëª© ê°•ì¡°
        self.reward_ax.set_xlabel('Timesteps')
        self.reward_ax.set_ylabel('í‰ê·  ë³´ìƒ')
        self.reward_ax.grid(True, alpha=0.3)
        self.reward_ax.legend()
        
        # 2. ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ë¶„ì„
        self.components_ax.clear()
        if self.reward_components_history:
            components_df = pd.DataFrame(self.reward_components_history)
            for col in components_df.columns[:5]:  # ìƒìœ„ 5ê°œë§Œ
                self.components_ax.plot(self.timesteps_history, 
                                      components_df[col], 
                                      label=col, linewidth=1.5)
            self.components_ax.set_title('ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ì¶”ì´', fontsize=12)
            self.components_ax.set_xlabel('Timesteps')
            self.components_ax.set_ylabel('ì»´í¬ë„ŒíŠ¸ ê°’')
            self.components_ax.legend(loc='best', fontsize=8)
            self.components_ax.grid(True, alpha=0.3)
        
        # 3. ì„±ê³µë¥ 
        self.success_ax.clear()
        self.success_ax.plot(self.timesteps_history, 
                           [r*100 for r in self.success_rates], 
                           'g-o', linewidth=2, markersize=6)
        if self.use_curriculum and self.curriculum_stages:
            # ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ í‘œì‹œ
            ax2 = self.success_ax.twinx()
            ax2.plot(self.timesteps_history, self.curriculum_stages, 
                    'orange', linestyle='--', linewidth=1)
            ax2.set_ylabel('ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„', color='orange')
            ax2.tick_params(axis='y', labelcolor='orange')
        self.success_ax.set_title('ì„±ê³µë¥  ì¶”ì´', fontsize=12)
        self.success_ax.set_xlabel('Timesteps')
        self.success_ax.set_ylabel('ì„±ê³µë¥  (%)')
        self.success_ax.set_ylim(0, 105)
        self.success_ax.grid(True, alpha=0.3)
        
        # 4. ì•ˆì •ì„± ë©”íŠ¸ë¦­
        self.stability_ax.clear()
        if self.stability_metrics:
            stability_df = pd.DataFrame(self.stability_metrics)
            x = self.timesteps_history[:len(stability_df)]
            for col in stability_df.columns:
                self.stability_ax.plot(x, stability_df[col], 
                                     label=col.replace('_', ' '), linewidth=1.5)
            self.stability_ax.set_title('ì•ˆì •ì„± ë©”íŠ¸ë¦­', fontsize=12)
            self.stability_ax.set_xlabel('Timesteps')
            self.stability_ax.set_ylabel('ì•ˆì •ì„± ì ìˆ˜')
            self.stability_ax.legend(loc='best', fontsize=8)
            self.stability_ax.grid(True, alpha=0.3)
            self.stability_ax.set_ylim(0, 1.1)
        
        # 5. ë³´ìƒ ì»´í¬ë„ŒíŠ¸ íˆíŠ¸ë§µ
        self.heatmap_ax.clear()
        if len(self.reward_components_history) > 5:
            # ìµœê·¼ ë°ì´í„°ë¡œ íˆíŠ¸ë§µ
            recent_components = pd.DataFrame(self.reward_components_history[-20:])
            if not recent_components.empty:
                data = recent_components.T.values
                im = self.heatmap_ax.imshow(data, aspect='auto', cmap='coolwarm')
                self.heatmap_ax.set_yticks(range(len(recent_components.columns)))
                self.heatmap_ax.set_yticklabels(recent_components.columns, fontsize=8)
                self.heatmap_ax.set_xlabel('ìµœê·¼ í‰ê°€ (ê³¼ê±° â†’ í˜„ì¬)')
                self.heatmap_ax.set_title('ë³´ìƒ ì»´í¬ë„ŒíŠ¸ íˆíŠ¸ë§µ', fontsize=12)
                plt.colorbar(im, ax=self.heatmap_ax, fraction=0.046, pad=0.04)
        
        # 6. ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        self.failure_ax.clear()
        if self.failure_reasons:
            # ì „ì²´ ì‹¤íŒ¨ ì›ì¸ ì§‘ê³„
            all_failures = {}
            for failure_dict in self.failure_reasons:
                for reason, count in failure_dict.items():
                    all_failures[reason] = all_failures.get(reason, 0) + count
            
            if all_failures:
                reasons = list(all_failures.keys())
                counts = list(all_failures.values())
                colors = plt.cm.Reds(np.linspace(0.4, 0.8, len(reasons)))
                self.failure_ax.pie(counts, labels=reasons, colors=colors, 
                                  autopct='%1.0f%%', startangle=90)
                self.failure_ax.set_title('ì‹¤íŒ¨ ì›ì¸ ë¶„í¬', fontsize=12)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitle ê³µê°„ í™•ë³´
        self.fig.suptitle('í–¥ìƒëœ í•™ìŠµ ì§„í–‰ ìƒí™© ë¦¬í¬íŠ¸', fontsize=16, weight='bold')
        plt.pause(0.1)
   
    def save_detailed_analysis(self, save_path: str):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        os.makedirs(save_path, exist_ok=True)
        
        # 1. ì»´í¬ë„ŒíŠ¸ ìƒê´€ê´€ê³„ ë¶„ì„
        if len(self.reward_components_history) > 10:
            components_df = pd.DataFrame(self.reward_components_history)
            
            plt.figure(figsize=(10, 8))
            corr = components_df.corr()
            plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)
            plt.colorbar()
            plt.xticks(range(len(corr.columns)), corr.columns, rotation=45)
            plt.yticks(range(len(corr.columns)), corr.columns)
            plt.title('ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ê°„ ìƒê´€ê´€ê³„')
            
            # ìƒê´€ê³„ìˆ˜ í‘œì‹œ
            for i in range(len(corr.columns)):
                for j in range(len(corr.columns)):
                    plt.text(j, i, f'{corr.iloc[i, j]:.2f}', 
                           ha='center', va='center',
                           color='white' if abs(corr.iloc[i, j]) > 0.5 else 'black')
            
            plt.tight_layout()
            plt.savefig(f"{save_path}/component_correlation.png", dpi=300)
            plt.close()
        
        # 2. í•™ìŠµ ë‹¨ê³„ë³„ ë¶„ì„
        if self.use_curriculum and self.curriculum_stages:
            plt.figure(figsize=(12, 8))
            
            # ìŠ¤í…Œì´ì§€ë³„ ì„±ê³µë¥ 
            stage_success = {}
            for i, stage in enumerate(self.curriculum_stages):
                if stage not in stage_success:
                    stage_success[stage] = []
                stage_success[stage].append(self.success_rates[i])
            
            plt.subplot(2, 1, 1)
            for stage, rates in stage_success.items():
                plt.bar(stage, np.mean(rates), alpha=0.7, 
                       label=f'Stage {stage}')
            plt.xlabel('ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„')
            plt.ylabel('í‰ê·  ì„±ê³µë¥ ')
            plt.title('ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ë³„ ì„±ê³µë¥ ')
            plt.legend()
            
            # ìŠ¤í…Œì´ì§€ ì§„í–‰ ì‹œê°„
            plt.subplot(2, 1, 2)
            plt.plot(self.timesteps_history, self.curriculum_stages, 'o-')
            plt.xlabel('Timesteps')
            plt.ylabel('ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„')
            plt.title('ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ì¶”ì´')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{save_path}/curriculum_analysis.png", dpi=300)
            plt.close()
        
        # 3. JSON í˜•ì‹ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ì €ì¥
        analysis_data = {
            'summary': {
                'total_evaluations': len(self.rewards_history),
                'final_reward': self.rewards_history[-1] if self.rewards_history else 0,
                'final_success_rate': self.success_rates[-1] if self.success_rates else 0,
                'best_reward': max(self.rewards_history) if self.rewards_history else 0,
                'best_success_rate': max(self.success_rates) if self.success_rates else 0,
            },
            'history': {
                'timesteps': self.timesteps_history,
                'rewards': self.rewards_history,
                'success_rates': self.success_rates,
                'episode_lengths': self.lengths_history,
            }
        }
        
        with open(f"{save_path}/analysis_data.json", 'w') as f:
            json.dump(analysis_data, f, indent=2)
        
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {save_path}")


class VideoRecordingCallback(BaseCallback):
    def __init__(
        self,
        record_env,
        record_interval_timesteps: int = 100_000,
        record_episodes: int = 1,
        video_folder: str = "training_videos",
        show_duration_seconds: int = 15,
    ):
        super().__init__()
        self.record_env = record_env
        self.record_interval = record_interval_timesteps
        self.record_episodes = record_episodes
        self.video_folder = video_folder
        self.show_duration_seconds = show_duration_seconds
        self.last_record_timestep = 0
        
        os.makedirs(video_folder, exist_ok=True)
        
        # moviepy ì„¤ì¹˜ í™•ì¸
        self.moviepy_available = check_and_install_moviepy()
        if not self.moviepy_available:
            print("âš ï¸ ë¹„ë””ì˜¤ ì €ì¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        print(f"ğŸ¥ ë¹„ë””ì˜¤ ë…¹í™” ì„¤ì •: {show_duration_seconds}ì´ˆê°„ ë…¹í™”")
    
    def _on_step(self) -> bool:
        if not self.moviepy_available:
            return True
            
        if self.num_timesteps - self.last_record_timestep >= self.record_interval:
            self._record_video()
            self.last_record_timestep = self.num_timesteps
        return True
    
    def _record_video(self):
        """ê°œì„ ëœ ë¹„ë””ì˜¤ ë…¹í™”"""
        print(f"\nğŸ¥ ë¹„ë””ì˜¤ ë…¹í™” ì¤‘... (Timestep: {self.num_timesteps:,})")
        
        try:
            # í™˜ê²½ ë¦¬ì…‹
            obs = self.record_env.reset()
            frames = []
            episode_reward = 0
            start_time = time.time()
            
            while time.time() - start_time < self.show_duration_seconds:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, info = self.record_env.step(action)
                episode_reward += reward[0] if isinstance(reward, np.ndarray) else reward
                
                # í”„ë ˆì„ ìº¡ì²˜
                frame = self.record_env.render(mode='rgb_array')
                if isinstance(frame, list):
                    frame = frame[0]
                frames.append(frame)
                
                if done[0] if isinstance(done, np.ndarray) else done:
                    obs = self.record_env.reset()
            
            # ë¹„ë””ì˜¤ ì €ì¥
            if frames:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{self.video_folder}/training_t{self.num_timesteps}_r{episode_reward:.0f}_{timestamp}.mp4"
                imageio.mimsave(filename, frames, fps=30)
                print(f"âœ… ë¹„ë””ì˜¤ ì €ì¥: {filename} (ë³´ìƒ: {episode_reward:.1f})")
                
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ ë…¹í™” ì‹¤íŒ¨: {str(e)}")

# ======================================================================================
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ======================================================================================
if __name__ == "__main__":
    
    # 1. í•™ìŠµì— ì‚¬ìš©í•  ë¡œë´‡ í™˜ê²½ ìƒì„±
    # Go1MujocoEnvì˜ ì„¤ì •ì€ í•„ìš”ì— ë”°ë¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    env = Go1MujocoEnv()

    # 2. ëª¨ë‹ˆí„°ë§ ì½œë°± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê°€ì¥ ê¸°ëŠ¥ì´ ë§ì€ EnhancedVisualCallback ì‚¬ìš©)
    # eval_interval_minutes: í‰ê°€ ì‹¤í–‰ ê°„ê²© (ë¶„ ë‹¨ìœ„)
    # show_duration_seconds: í‰ê°€ ì‹œ í•œ ì—í”¼ì†Œë“œë‹¹ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ (ì´ˆ)
    visual_callback = EnhancedVisualCallback(
        eval_env=env, 
        eval_interval_minutes=10,  # 10ë¶„ë§ˆë‹¤ í‰ê°€
        show_duration_seconds=20   # 20ì´ˆ ë™ì•ˆ ì‹œë®¬ë ˆì´ì…˜
    )

    # 3. ê°•í™”í•™ìŠµ ëª¨ë¸ ìƒì„± (PPO ì•Œê³ ë¦¬ì¦˜)
    # policy="MlpPolicy": ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (ì‹ ê²½ë§) ì •ì±… ì‚¬ìš©
    # verbose=1: í•™ìŠµ ì§„í–‰ ìƒí™©ì„ í„°ë¯¸ë„ì— ì¶œë ¥
    model = PPO("MlpPolicy", env, verbose=1)

    print("====================== í•™ìŠµ ì‹œì‘ ======================")
    
    try:
        # 4. ëª¨ë¸ í•™ìŠµ ì‹œì‘
        # total_timesteps: ì´ í•™ìŠµí•  íšŸìˆ˜(íƒ€ì„ìŠ¤í…)
        # callback: í•™ìŠµ ì¤‘ê°„ì— í˜¸ì¶œí•  ì½œë°± ì§€ì •
        model.learn(
            total_timesteps=2_000_000,
            callback=visual_callback
        )
    finally:
        # 5. í•™ìŠµì´ ì¤‘ë‹¨ë˜ê±°ë‚˜ ì™„ë£Œë˜ë©´, ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥
        print("\n====================== í•™ìŠµ ì¢…ë£Œ ======================")
        save_directory = "./training_reports"
        visual_callback.save_detailed_analysis(save_path=save_directory)
        print(f"ìµœì¢… ë¦¬í¬íŠ¸ê°€ '{save_directory}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")