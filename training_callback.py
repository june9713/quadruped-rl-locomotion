#!/usr/bin/env python3
import threading
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv
import gymnasium as gym
from collections import deque, defaultdict
import os
import time
import imageio.v2 as imageio
import pandas as pd
import datetime
from go1_mujoco_env import Go1MujocoEnv
import copy
import subprocess
import sys
import json
from matplotlib.gridspec import GridSpec
import queue # <--- ì´ ë¼ì¸ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.



class CurriculumCallback(BaseCallback):
    """
    í•™ìŠµ ì§„í–‰ë¥ ì— ë”°ë¼ í™˜ê²½ì˜ 'rand_power'ë¥¼ ì¡°ì ˆí•˜ëŠ” ì»¤ë¦¬í˜ëŸ¼ ì½œë°±ì…ë‹ˆë‹¤.

    ì´ ì½œë°±ì€ í•™ìŠµ ì‹œì‘ ì‹œ ì‚¬ìš©ìê°€ ì§€ì •í•œ 'rand_power' ê°’ì—ì„œ ì‹œì‘í•˜ì—¬,
    ì „ì²´ í•™ìŠµ timestepsì˜ 70% ì§€ì ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œì‹œì¼œ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    70% ì´í›„ë¶€í„°ëŠ” 'rand_power'ë¥¼ 0ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ ì•ˆì •ì ì¸ ì •ì±…ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.
    """
    def __init__(self, total_timesteps: int, initial_rand_power: float, verbose: int = 0):
        """
        CurriculumCallback ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        :param total_timesteps: ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë  ì´ íƒ€ì„ìŠ¤í… ìˆ˜.
        :param initial_rand_power: í•™ìŠµ ì‹œì‘ ì‹œ ì ìš©í•  ì´ˆê¸° ëœë¤í™” ê°•ë„.
        :param verbose: ìƒì„¸ ì •ë³´ ì¶œë ¥ ë ˆë²¨.
        """
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.initial_rand_power = initial_rand_power
        # ì»¤ë¦¬í˜ëŸ¼ì´ ì¢…ë£Œë˜ëŠ” ì‹œì  (ì „ì²´ í•™ìŠµì˜ 70%)
        self.curriculum_end_step = int(total_timesteps * 0.7)
        self._last_logged_power = -1.0

    def _on_step(self) -> bool:
        """
        í•™ìŠµì˜ ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œë˜ì–´ rand_powerë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•©ë‹ˆë‹¤.
        """
        current_step = self.num_timesteps

        if current_step < self.curriculum_end_step:
            # 70% ì§€ì ì— ë„ë‹¬í•  ë•Œê¹Œì§€ rand_powerë¥¼ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œì‹œí‚µë‹ˆë‹¤.
            # ì§„í–‰ë¥  (0.0 ~ 1.0) ê³„ì‚°
            progress = current_step / self.curriculum_end_step
            new_rand_power = self.initial_rand_power * (1.0 - progress)
        else:
            # 70% ì§€ì ì„ ë„˜ì–´ì„œë©´ rand_powerë¥¼ 0ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
            new_rand_power = 0.0

        # ëª¨ë“  ë³‘ë ¬ í™˜ê²½ì— ìƒˆë¡œìš´ rand_power ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        # Go1MujocoEnv í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œëŠ” '_rand_power'ë¼ëŠ” ì´ë¦„ì˜ ì†ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        self.training_env.set_attr("_rand_power", new_rand_power)

        # rand_power ê°’ì˜ ë³€í™”ë¥¼ TensorBoardì— ë¡œê¹…í•©ë‹ˆë‹¤.
        self.logger.record("curriculum/rand_power", new_rand_power)
        
        return True


def check_and_install_moviepy():
    """moviepy ì„¤ì¹˜ í™•ì¸ ë° ìë™ ì„¤ì¹˜"""
    try:
        import moviepy
        print("âœ… moviepyê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return True
    except ImportError:
        print("ğŸ“¦ moviepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "moviepy"])
            print("âœ… moviepy ì„¤ì¹˜ ì™„ë£Œ!")
            return True
        except subprocess.CalledProcessError:
            print("âŒ moviepy ì„¤ì¹˜ ì‹¤íŒ¨. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install moviepy")
            return False


class VisualTrainingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ê°„ì— ì‹œê°ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì½œë°±"""
    
    def __init__(
        self,
        eval_env: gym.Env,
        eval_freq: int = 300_000,
        n_eval_episodes: int = 3,
        show_duration_seconds: int = 30,
        save_videos: bool = True,
        verbose: int = 1,
    ):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.show_duration_seconds = show_duration_seconds
        self.step_zero = True
        
        self.last_eval_timestep = 0
        self.eval_count = 0
        self.performance_history = deque(maxlen=50)
        
        # ë°ì´í„° ì €ì¥ìš©
        self.rewards_history = []
        self.lengths_history = []
        self.success_rates = []
        self.timesteps_history = []
        
    def _on_step(self) -> bool:
        # eval_freq ê°„ê²©ìœ¼ë¡œ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë©°, í•™ìŠµ ì‹œì‘ ì‹œì (self.step_zero)ì—ë„ ì²« í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        if self.step_zero or (self.num_timesteps - self.last_eval_timestep >= self.eval_freq):
            self.step_zero = False
            self._evaluate_and_visualize()
            self.last_eval_timestep = self.num_timesteps
            
        return True
    
    def _evaluate_and_visualize(self):
        """ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™” + MP4 ì €ì¥"""
        self.eval_count += 1

        print(f"\n{'='*60}")
        print(f"ğŸ“Š í‰ê°€ #{self.eval_count} (Timestep: {self.num_timesteps:,})")
        print(f"â° ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {self.show_duration_seconds}ì´ˆ")
        print(f"{'='*60}")

        episode_rewards = []
        episode_lengths = []
        success_count = 0
        self.n_eval_episodes  =2
        for episode in range(self.n_eval_episodes):
            print(f"  ğŸ® ì—í”¼ì†Œë“œ {episode + 1}/{self.n_eval_episodes} ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")

            obs, _ = self.eval_env.reset()
            episode_reward = 0.0
            episode_length = 0
            frames = []

            start_time = time.time()

            while True:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)

                episode_reward += float(reward)
                episode_length += 1

                # í”„ë ˆì„ ìº¡ì²˜
                frame = None
                try:
                    frame = self.eval_env.render()
                    if isinstance(frame, list):
                        frame = frame[0]
                except Exception:
                    try:
                        frame = self.eval_env.render(mode="rgb_array")
                    except Exception:
                        frame = None

                if frame is not None:
                    frames.append(frame)

                time.sleep(0.02)

                if time.time() - start_time >= self.show_duration_seconds:
                    print(f"    â° ì‹œê°„ ì œí•œ ({self.show_duration_seconds}ì´ˆ) ë„ë‹¬")
                    break

                if terminated or truncated:
                    if info.get('bipedal_success', False):  # <-- ìˆ˜ì •ëœ ì½”ë“œ
                        success_count += 1
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            print(f"    ğŸ“ˆ ë³´ìƒ: {episode_reward:.2f}, ê¸¸ì´: {episode_length}")

        # ê²°ê³¼ ì§‘ê³„
        mean_r, std_r = np.mean(episode_rewards), np.std(episode_rewards)
        mean_l, std_l = np.mean(episode_lengths), np.std(episode_lengths)
        success_rate = success_count / self.n_eval_episodes

        self.rewards_history.append(mean_r)
        self.lengths_history.append(mean_l)
        self.success_rates.append(success_rate)
        self.timesteps_history.append(self.num_timesteps)

        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"  í‰ê·  ë³´ìƒ: {mean_r:.2f} Â± {std_r:.2f}")
        print(f"  í‰ê·  ê¸¸ì´: {mean_l:.1f} Â± {std_l:.1f}")
        print(f"  ì„±ê³µë¥ : {success_rate:.1%} ({success_count}/{self.n_eval_episodes})")
        print(f"{'='*60}\n")

        self._update_plots()
   
    def _update_plots(self):
        pass
        
   
    def save_progress_report(self, save_path: str):
        """ì§„í–‰ ìƒí™© ë³´ê³ ì„œ ì €ì¥"""
        if not self.rewards_history:
            return
            
        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
        plt.figure(figsize=(15, 10))
        
        # 2x2 ì„œë¸Œí”Œë¡¯
        plt.subplot(2, 2, 1)
        plt.plot(self.timesteps_history, self.rewards_history, 'b-o', linewidth=2)
        plt.title('í•™ìŠµ ì§„í–‰: í‰ê·  ë³´ìƒ', fontsize=14)
        plt.xlabel('Timesteps')
        plt.ylabel('í‰ê·  ë³´ìƒ')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 2)
        plt.plot(self.timesteps_history, self.lengths_history, 'g-o', linewidth=2)
        plt.title('í•™ìŠµ ì§„í–‰: í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´', fontsize=14)
        plt.xlabel('Timesteps')
        plt.ylabel('í‰ê·  ê¸¸ì´')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 3)
        plt.plot(self.timesteps_history, [r*100 for r in self.success_rates], 'r-o', linewidth=2)
        plt.title('í•™ìŠµ ì§„í–‰: ì„±ê³µë¥ ', fontsize=14)
        plt.xlabel('Timesteps')
        plt.ylabel('ì„±ê³µë¥  (%)')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 100)
        
        plt.subplot(2, 2, 4)
        if len(self.rewards_history) > 5:
            # í•™ìŠµ ê³¡ì„ ì˜ ê¸°ìš¸ê¸° (ê°œì„  ì†ë„)
            improvement_rate = np.diff(self.rewards_history)
            plt.plot(self.timesteps_history[1:], improvement_rate, 'purple', linewidth=2)
            plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
            plt.title('í•™ìŠµ ê°œì„  ì†ë„', fontsize=14)
            plt.xlabel('Timesteps')
            plt.ylabel('ë³´ìƒ ê°œì„ ëŸ‰')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_path}/training_progress.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # CSVë¡œ ë°ì´í„° ì €ì¥
        df = pd.DataFrame({
            'timesteps': self.timesteps_history,
            'mean_reward': self.rewards_history,
            'mean_length': self.lengths_history,
            'success_rate': self.success_rates
        })
        df.to_csv(f"{save_path}/training_data.csv", index=False)
        
        print(f"ğŸ“Š ì§„í–‰ ìƒí™© ë³´ê³ ì„œ ì €ì¥: {save_path}")


# íŒŒì¼ëª…: training_callback.py

class EnhancedVisualCallback(VisualTrainingCallback):
    """
    ê°œì„ ëœ ì‹œê°í™” ì½œë°± - ì‹¤ì‹œê°„ ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, *args, use_curriculum=False, best_model_save_path: str = None, load_history_from: str = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.use_curriculum = use_curriculum
        
        # ìµœê³  ëª¨ë¸ ì €ì¥ì„ ìœ„í•œ ì„¤ì •
        self.best_model_save_path = best_model_save_path
        self.best_mean_reward = -np.inf
        if self.best_model_save_path is not None:
            os.makedirs(self.best_model_save_path, exist_ok=True)

        # ì¶”ê°€ ì¶”ì  ë°ì´í„°
        self.reward_components_history = []
        self.curriculum_stages = []
        self.stability_metrics = []
        self.failure_reasons = []
        self.explained_variance_history = []
        self.explained_variance_timesteps = []
        
        # --- ìŠ¤ë ˆë”© ê´€ë ¨ ì½”ë“œ ì œê±° ---
        print("\nğŸ“ˆ ì‹¤ì‹œê°„ í•™ìŠµ ê·¸ë˜í”„ëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ì— 'realtime_progress.png' íŒŒì¼ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.")
        
        # âœ¨ [ì¶”ê°€] í•™ìŠµ ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
        if load_history_from:
            self._load_history(load_history_from)

    def _load_history(self, path: str):
        """ì €ì¥ëœ í•™ìŠµ ê¸°ë¡ì„ .json íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
        if not os.path.exists(path):
            print(f"âš ï¸ í•™ìŠµ ê¸°ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}. ìƒˆë¡œìš´ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            return
        
        try:
            with open(path, 'r') as f:
                history = json.load(f)
            
            # ì´ì „ ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
            self.best_mean_reward = history.get('best_mean_reward', -np.inf)
            self.rewards_history = history.get('rewards_history', [])
            self.lengths_history = history.get('lengths_history', [])
            self.success_rates = history.get('success_rates', [])
            self.timesteps_history = history.get('timesteps_history', [])
            self.reward_components_history = history.get('reward_components_history', [])
            self.stability_metrics = history.get('stability_metrics', [])
            self.failure_reasons = history.get('failure_reasons', [])
            
            # eval_countëŠ” ê¸°ë¡ëœ ë°ì´í„°ì˜ ê¸¸ì´ë¡œ ì„¤ì •
            self.eval_count = len(self.rewards_history)
            
            if self.timesteps_history:
                print(f"âœ… í•™ìŠµ ê¸°ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {path}")
                print(f"   - {self.eval_count}ê°œì˜ ì´ì „ í‰ê°€ ì§€ì ì—ì„œ ì´ì–´ê°‘ë‹ˆë‹¤.")
                print(f"   - ë§ˆì§€ë§‰ Timestep: {self.timesteps_history[-1]:,}, ìµœê³  ë³´ìƒ: {self.best_mean_reward:.2f}")
            else:
                print("   - ë¶ˆëŸ¬ì˜¨ ê¸°ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ í•™ìŠµ ê¸°ë¡ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}. ìƒˆë¡œìš´ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
   
    def _evaluate_and_visualize(self):
        """ê°œì„ ëœ í‰ê°€ ë° ì‹œê°í™”"""
        self.eval_count += 1
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ê³ ê¸‰ í‰ê°€ #{self.eval_count} (Timestep: {self.num_timesteps:,})")
        print(f"{'='*70}")
        
        # ê¸°ë³¸ í‰ê°€ ë°ì´í„°
        episode_rewards = []
        episode_lengths = []
        episode_components = []
        episode_stability = []
        episode_failures = []
        success_count = 0
        self.n_eval_episodes = 2
        for episode in range(self.n_eval_episodes):
            print(f"\nğŸ® ì—í”¼ì†Œë“œ {episode + 1}/{self.n_eval_episodes}")
            
            obs, _ = self.eval_env.reset()
            episode_reward = 0.0
            episode_length = 0
            
            # ì—í”¼ì†Œë“œë³„ ìƒì„¸ ì¶”ì 
            reward_components = {}
            stability_metrics = []
            frames = []
            
            start_time = time.time()
            
            while True:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)
                
                episode_reward += float(reward)
                episode_length += 1
                
                # ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ì¶”ì 
                if 'upright' in info:
                    for key in ['upright', 'height', 'feet', 'forward_vel', 
                               'lateral_vel', 'cop_stab', 'zmp_stab']:
                        if key in info:
                            if key not in reward_components:
                                reward_components[key] = []
                            reward_components[key].append(info.get(key, 0))
                
                # ì•ˆì •ì„± ë©”íŠ¸ë¦­
                if 'stab_ang' in info:
                    stability_metrics.append({
                        'angular_stability': info.get('stab_ang', 0),
                        'cop_stability': info.get('cop_stab', 0),
                        'zmp_stability': info.get('zmp_stab', 0)
                    })
                
                # í”„ë ˆì„ ìº¡ì²˜
                #if self.save_videos:
                #    try:
                #        frame = self.eval_env.render()
                #        if frame is not None:
                #            frames.append(frame)
                #    except:
               #         pass
                
                time.sleep(0.01)
                
                # ì¢…ë£Œ ì¡°ê±´
                if time.time() - start_time >= self.show_duration_seconds:
                    break
                    
                if terminated or truncated:
                    if info.get('bipedal_success', False):
                        success_count += 1
                    else:
                        failure_reason = self._analyze_failure(info, obs)
                        episode_failures.append(failure_reason)
                    break
            
            # ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            avg_components = {}
            for key, values in reward_components.items():
                if values:
                    avg_components[key] = np.mean(values)
            episode_components.append(avg_components)
            
            if stability_metrics:
                avg_stability = {
                    key: np.mean([m[key] for m in stability_metrics])
                    for key in stability_metrics[0].keys()
                }
                episode_stability.append(avg_stability)
            
            print(f"  ğŸ“ˆ ë³´ìƒ: {episode_reward:.2f}")
            print(f"  â±ï¸ ê¸¸ì´: {episode_length}")
            print(f"  ğŸ¯ ì£¼ìš” ì»´í¬ë„ŒíŠ¸: {', '.join([f'{k}:{v:.2f}' for k,v in avg_components.items()][:3])}")
            
            #if self.save_videos and frames: #SAVE VIDEO ì‚¬ìš©í•œí•˜ê² ìŒ!!!!!! ì¶”ê°€í•˜ì§€ ë§ˆì„¸ì˜¤!!!
            #    self._save_video(frames, episode, episode_reward)
        
        # ì „ì²´ í‰ê°€ ê²°ê³¼ ì €ì¥ ë° ìµœê³  ëª¨ë¸ ì—…ë°ì´íŠ¸
        self._update_history(episode_rewards, episode_lengths, 
                            episode_components, episode_stability, 
                            episode_failures, success_count)
        
        # í”Œë¡¯ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
        self._update_enhanced_plots()
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š í‰ê°€ ìš”ì•½:")
        print(f"  í‰ê·  ë³´ìƒ: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
        print(f"  ì„±ê³µë¥ : {success_count/self.n_eval_episodes:.1%}")
        if self.use_curriculum and hasattr(self.eval_env, 'standing_reward'):
            print(f"  ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„: {self.eval_env.standing_reward.curriculum_stage}")
        print(f"{'='*70}\n")
   
    def _analyze_failure(self, info, obs):
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        reasons = []
        
        if info.get('upright', 1) < 0.5:
            reasons.append('fall_forward' if obs[0] > 0 else 'fall_backward')
        if info.get('height', 1) < 0.3:
            reasons.append('too_low')
        if info.get('lateral_vel', 1) < 0.5:
            reasons.append('lateral_instability')
        if info.get('joint_limit', 0) < -0.5:
            reasons.append('joint_limit_violation')
        
        return reasons[0] if reasons else 'unknown'
   
    def _save_video(self, frames, episode, reward):
        """ë¹„ë””ì˜¤ ì €ì¥"""
        os.makedirs("eval_videos", exist_ok=True)
        filename = (f"eval_videos/enhanced_eval{self.eval_count}_ep{episode+1}_"
                   f"r{reward:.0f}_t{self.num_timesteps}.mp4")
        try:
            imageio.mimsave(filename, frames, fps=30)
        except:
            pass
   
    def _update_history(self, rewards, lengths, components, stability, failures, successes):
        """íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        mean_reward = np.mean(rewards)
        self.rewards_history.append(mean_reward)
        self.lengths_history.append(np.mean(lengths))
        self.success_rates.append(successes / self.n_eval_episodes)
        self.timesteps_history.append(self.num_timesteps)
        
        if self.best_model_save_path is not None:
            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward
                print(f"\nğŸš€ ìƒˆë¡œìš´ ìµœê³  í‰ê·  ë³´ìƒ ë‹¬ì„±: {self.best_mean_reward:.2f} (Timestep: {self.num_timesteps:,})")
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                save_path = os.path.join(self.best_model_save_path, "best_model.zip")
                self.model.save(save_path)
                print(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")

                # âœ¨ [ì¶”ê°€] í•™ìŠµ ê¸°ë¡ ì €ì¥ ë¡œì§
                history_data = {
                    'best_mean_reward': self.best_mean_reward,
                    'rewards_history': self.rewards_history,
                    'lengths_history': self.lengths_history,
                    'success_rates': self.success_rates,
                    'timesteps_history': self.timesteps_history,
                    'reward_components_history': self.reward_components_history,
                    'stability_metrics': self.stability_metrics,
                    'failure_reasons': self.failure_reasons,
                }
                history_save_path = os.path.join(self.best_model_save_path, "training_history.json")
                try:
                    with open(history_save_path, 'w') as f:
                        # numpy íƒ€ì…ì„ python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                        json.dump(history_data, f, indent=4, default=float)
                    print(f"ğŸ’¾ í•™ìŠµ ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {history_save_path}")
                except Exception as e:
                    print(f"âŒ í•™ìŠµ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")


        avg_components = {}
        if components:
            keys = components[0].keys()
            for key in keys:
                values = [c.get(key, 0) for c in components]
                avg_components[key] = np.mean(values)
        self.reward_components_history.append(avg_components)
        
        if stability:
            avg_stability = {}
            keys = stability[0].keys()
            for key in keys:
                values = [s.get(key, 0) for s in stability]
                avg_stability[key] = np.mean(values)
            self.stability_metrics.append(avg_stability)
        
        failure_counts = {}
        for f in failures:
            failure_counts[f] = failure_counts.get(f, 0) + 1
        self.failure_reasons.append(failure_counts)
        
        if self.use_curriculum and hasattr(self.eval_env, 'standing_reward'):
            self.curriculum_stages.append(self.eval_env.standing_reward.curriculum_stage)
   
    def _update_enhanced_plots(self):
        """ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµ ì§„í–‰ ìƒí™© ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if len(self.rewards_history) < 2:
            return

        fig = None  # ì˜ˆì™¸ ë°œìƒ ì‹œ fig ë³€ìˆ˜ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ˆê¸°í™”
        try:
            fig, ax = plt.subplots(figsize=(12, 7))
            timesteps = self.timesteps_history
            rewards = self.rewards_history

            ax.plot(timesteps, rewards, 'b-', linewidth=2, label='í‰ê·  ë³´ìƒ')

            # ì´ë™ í‰ê· ì„  ì¶”ê°€
            if len(rewards) >= 10:
                window = 10
                ma = np.convolve(rewards, np.ones(window)/window, mode='valid')
                ax.plot(timesteps[window-1:], ma, 'r--', linewidth=2, label=f'ì´ë™í‰ê·  ({window}-evals)')

            ax.set_title('ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™©: í‰ê·  ë³´ìƒ', fontsize=16)
            ax.set_xlabel('Timesteps')
            ax.set_ylabel('í‰ê·  ë³´ìƒ')
            ax.grid(True, alpha=0.4)
            ax.legend()
            fig.tight_layout()

            save_path = "./training_progress.png"
            plt.savefig(save_path, dpi=100)
            
        except Exception as e:
            print(f"âŒ ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            if fig is not None:
                plt.close(fig) # ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ í•­ìƒ figureë¥¼ ë‹«ìŒ

    def save_detailed_analysis(self, save_path: str):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        os.makedirs(save_path, exist_ok=True)
        
        if len(self.reward_components_history) > 10:
            components_df = pd.DataFrame(self.reward_components_history)
            
            plt.figure(figsize=(10, 8))
            corr = components_df.corr()
            plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)
            plt.colorbar()
            plt.xticks(range(len(corr.columns)), corr.columns, rotation=45)
            plt.yticks(range(len(corr.columns)), corr.columns)
            plt.title('ë³´ìƒ ì»´í¬ë„ŒíŠ¸ ê°„ ìƒê´€ê´€ê³„')
            
            for i in range(len(corr.columns)):
                for j in range(len(corr.columns)):
                    plt.text(j, i, f'{corr.iloc[i, j]:.2f}', 
                           ha='center', va='center',
                           color='white' if abs(corr.iloc[i, j]) > 0.5 else 'black')
            
            plt.tight_layout()
            plt.savefig(f"{save_path}/component_correlation.png", dpi=300)
            plt.close()
        
        if self.use_curriculum and self.curriculum_stages:
            plt.figure(figsize=(12, 8))
            
            stage_success = {}
            for i, stage in enumerate(self.curriculum_stages):
                if stage not in stage_success:
                    stage_success[stage] = []
                stage_success[stage].append(self.success_rates[i])
            
            plt.subplot(2, 1, 1)
            for stage, rates in stage_success.items():
                plt.bar(stage, np.mean(rates), alpha=0.7, 
                       label=f'Stage {stage}')
            plt.xlabel('ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„')
            plt.ylabel('í‰ê·  ì„±ê³µë¥ ')
            plt.title('ì»¤ë¦¬í˜í˜ëŸ¼ ë‹¨ê³„ë³„ ì„±ê³µë¥ ')
            plt.legend()
            
            plt.subplot(2, 1, 2)
            plt.plot(self.timesteps_history, self.curriculum_stages, 'o-')
            plt.xlabel('Timesteps')
            plt.ylabel('ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„')
            plt.title('ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ ì¶”ì´')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{save_path}/curriculum_analysis.png", dpi=300)
            plt.close()
        
        analysis_data = {
            'summary': {
                'total_evaluations': len(self.rewards_history),
                'final_reward': self.rewards_history[-1] if self.rewards_history else 0,
                'final_success_rate': self.success_rates[-1] if self.success_rates else 0,
                'best_reward': max(self.rewards_history) if self.rewards_history else 0,
                'best_success_rate': max(self.success_rates) if self.success_rates else 0,
            },
            'history': {
                'timesteps': self.timesteps_history,
                'rewards': self.rewards_history,
                'success_rates': self.success_rates,
                'episode_lengths': self.lengths_history,
            }
        }
        
        with open(f"{save_path}/analysis_data.json", 'w') as f:
            json.dump(analysis_data, f, indent=2)
        
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {save_path}")


class VideoRecordingCallback(BaseCallback):
    def __init__(
        self,
        record_env,
        record_interval_timesteps: int = 100_000,
        record_episodes: int = 1,
        video_folder: str = "training_videos",
        show_duration_seconds: int = 15,
    ):
        super().__init__()
        self.record_env = record_env
        self.record_interval = record_interval_timesteps
        self.record_episodes = record_episodes
        self.video_folder = video_folder
        self.show_duration_seconds = show_duration_seconds
        self.last_record_timestep = 0
        
        os.makedirs(video_folder, exist_ok=True)
        
        # moviepy ì„¤ì¹˜ í™•ì¸
        self.moviepy_available = check_and_install_moviepy()
        if not self.moviepy_available:
            print("âš ï¸ ë¹„ë””ì˜¤ ì €ì¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        
        print(f"ğŸ¥ ë¹„ë””ì˜¤ ë…¹í™” ì„¤ì •: {show_duration_seconds}ì´ˆê°„ ë…¹í™”")
    
    # íŒŒì¼ëª…: training_callback.py -> í´ë˜ìŠ¤ëª…: VideoRecordingCallback

    def _on_step(self) -> bool:
        if not self.moviepy_available:
            return True
            
        if self.num_timesteps - self.last_record_timestep >= self.record_interval:
            self._record_video()
            self.last_record_timestep = self.num_timesteps
        return True
    
    def _record_video(self):
        """ì›í•˜ëŠ” ê¸¸ì´ì˜ ë¹„ë””ì˜¤ë¥¼ ì •í™•íˆ ë…¹í™”í•˜ë„ë¡ ìˆ˜ì •í•œ í•¨ìˆ˜"""
        print(f"\nğŸ¥ ë¹„ë””ì˜¤ ë…¹í™” ì¤‘... (Timestep: {self.num_timesteps:,})")
        
        termination_counts = defaultdict(int)
        total_terminations_in_video = 0
        
        try:
            obs, _ = self.record_env.reset()
            frames = []
            episode_reward = 0

            # ëª©í‘œ ì˜ìƒ ê¸¸ì´ì™€ FPS ì„¤ì •
            target_video_seconds = self.show_duration_seconds
            fps = 30
            num_frames_to_record = target_video_seconds * fps
            
            # ì‹œê°„ ê¸°ë°˜ ë£¨í”„ë¥¼ í”„ë ˆì„ ìˆ˜ ê¸°ë°˜ ë£¨í”„ë¡œ ë³€ê²½
            while len(frames) < num_frames_to_record:
                
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.record_env.step(action)
                
                episode_reward += reward[0] if isinstance(reward, np.ndarray) else reward
                
                frame = self.record_env.render()
                if isinstance(frame, list):
                    frame = frame[0]
                frames.append(frame)
                
                is_done = terminated or truncated
                if is_done:
                    current_info = info[0] if isinstance(info, list) else info
                    reason = current_info.get('termination_reason')
                    
                    if reason and reason != 'not_terminated':
                        base_reason = reason.split(' (')[0]
                        termination_counts[base_reason] += 1
                        total_terminations_in_video += 1

                        # âœ¨ ì¶”ê°€ëœ ë¶€ë¶„: ìƒì„¸í•œ ì¢…ë£Œ ì›ì¸ê³¼ ê°’ì„ ì§ì ‘ ì¶œë ¥
                        details = current_info.get('termination_details', 'ì„¸ë¶€ ì •ë³´ ì—†ìŒ.')
                        #print(f"      - ì¢…ë£Œ ë°œìƒ: {base_reason}")
                        #print(f"        â””> ìƒì„¸ ì •ë³´: {details}")
                    
                    obs, _ = self.record_env.reset()
            
            if frames:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                # íŒŒì¼ëª…ì—ì„œ ë³´ìƒ ê°’ì„ ì†Œìˆ˜ì  ì—†ì´ ì •ìˆ˜ë¡œ í‘œì‹œí•˜ë„ë¡ ìˆ˜ì •
                filename = f"{self.video_folder}/training_t{self.num_timesteps}_r{int(episode_reward)}_{timestamp}.mp4"
                imageio.mimsave(filename, frames, fps=fps) # ì„¤ì •í•œ fps ê°’ ì‚¬ìš©
                print(f"âœ… ë¹„ë””ì˜¤ ì €ì¥: {filename} (ë³´ìƒ: {episode_reward:.1f})")

            if total_terminations_in_video > 0:
                print("-----------------------------------------")
                print("| Termination Reasons (During Video)    |")
                print(f"| Total terminations: {total_terminations_in_video:<16} |")
                print("-----------------------------------------")
                
                sorted_reasons = sorted(termination_counts.items(), key=lambda item: item[1], reverse=True)

                for reason, count in sorted_reasons:
                    percentage = (count / total_terminations_in_video) * 100
                    print(f"| {reason:<25} | {count:<5} ({percentage:>5.1f}%) |")
                print("-----------------------------------------")

        except Exception as e:
            import traceback
            print(f"âŒ ë¹„ë””ì˜¤ ë…¹í™” ì‹¤íŒ¨: {str(e)}")
            traceback.print_exc()


class RealTimeSavingCallback(BaseCallback):
    """
    ì‹¤ì‹œê°„ ì €ì¥ì„ ìœ„í•œ ì½œë°± - í™˜ê²½ ìƒíƒœ, í•™ìŠµ ë©”íŠ¸ë¦­, ì„¤ì •ê°’ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    ê¸°ì¡´ ì½œë°±ë“¤ê³¼ ì™„ë²½íˆ í˜¸í™˜ë˜ë©°, ê¸°ì¡´ ê¸°ëŠ¥ì€ ì „í˜€ ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        save_dir: str = "realtime_data",
        save_frequency: int = 1000,  # ë§¤ 1000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
        save_episode_data: bool = True,  # ì—í”¼ì†Œë“œë³„ ìƒì„¸ ë°ì´í„° ì €ì¥
        save_environment_state: bool = True,  # í™˜ê²½ ìƒíƒœ ì €ì¥
        save_hyperparameters: bool = True,  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
        save_checkpoints: bool = True,  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_frequency: int = 10000,  # ë§¤ 10000 ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸
        total_timesteps: int = None,  # ì „ì²´ í•™ìŠµ íƒ€ì„ìŠ¤í… ìˆ˜
        verbose: int = 1,
    ):
        super().__init__(verbose)
        self.save_dir = save_dir
        self.save_frequency = save_frequency
        self.save_episode_data = save_episode_data
        self.save_environment_state = save_environment_state
        self.save_hyperparameters = save_hyperparameters
        self.save_checkpoints = save_checkpoints
        self.checkpoint_frequency = checkpoint_frequency
        self.total_timesteps = total_timesteps  # ì „ì²´ í•™ìŠµ íƒ€ì„ìŠ¤í… ìˆ˜ ì €ì¥
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(f"{self.save_dir}/episodes", exist_ok=True)
        os.makedirs(f"{self.save_dir}/environment_states", exist_ok=True)
        os.makedirs(f"{self.save_dir}/checkpoints", exist_ok=True)
        os.makedirs(f"{self.save_dir}/hyperparameters", exist_ok=True)
        os.makedirs(f"{self.save_dir}/learning_metrics", exist_ok=True)  # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥ ë””ë ‰í† ë¦¬ ì¶”ê°€
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥ìš© ë³€ìˆ˜ë“¤
        self.episode_data = []
        self.environment_states = []
        self.learning_metrics = []
        self.last_save_step = 0
        self.last_checkpoint_step = 0
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ (í•™ìŠµ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
        self.hyperparameters_saved = False
        
        if verbose > 0:
            print(f"ğŸ”§ ì‹¤ì‹œê°„ ì €ì¥ ì½œë°± ì´ˆê¸°í™” ì™„ë£Œ: {self.save_dir}")
            print(f"   - ì €ì¥ ì£¼ê¸°: {self.save_frequency} ìŠ¤í…")
            print(f"   - ì²´í¬í¬ì¸íŠ¸ ì£¼ê¸°: {self.checkpoint_frequency} ìŠ¤í…")
            if total_timesteps:
                print(f"   - ì „ì²´ í•™ìŠµ íƒ€ì„ìŠ¤í…: {total_timesteps:,}")
    
    def _on_training_start(self) -> None:
        """í•™ìŠµ ì‹œì‘ ì‹œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥"""
        if self.save_hyperparameters and not self.hyperparameters_saved:
            self._save_hyperparameters()
            self.hyperparameters_saved = True
    
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œë˜ì–´ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"""
        # âœ¨ [ìˆ˜ì •] modelì´ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ë°ì´í„° ìˆ˜ì§‘
        if not hasattr(self, 'model') or self.model is None:
            return True
        
        # ì—í”¼ì†Œë“œ ë°ì´í„° ìˆ˜ì§‘
        if self.save_episode_data:
            self._collect_episode_data()
        
        # í™˜ê²½ ìƒíƒœ ìˆ˜ì§‘
        if self.save_environment_state:
            self._collect_environment_state()
        
        # âœ¨ [ì‹ ê·œ ì¶”ê°€] í•™ìŠµ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self._collect_learning_metrics()
        
        # ì£¼ê¸°ì  ì €ì¥
        if self.num_timesteps - self.last_save_step >= self.save_frequency:
            self._save_realtime_data()
            self.last_save_step = self.num_timesteps
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (self.save_checkpoints and 
            self.num_timesteps - self.last_checkpoint_step >= self.checkpoint_frequency):
            self._save_checkpoint()
            self.last_checkpoint_step = self.num_timesteps
        
        return True
    
    def _collect_episode_data(self):
        """ì—í”¼ì†Œë“œë³„ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ - ê°œì„ ëœ ë²„ì „"""
        # í˜„ì¬ í™˜ê²½ì—ì„œ ì—í”¼ì†Œë“œ ì •ë³´ ìˆ˜ì§‘
        if hasattr(self.training_env, 'get_attr'):
            try:
                # ë³‘ë ¬ í™˜ê²½ì—ì„œ ì—í”¼ì†Œë“œ ì •ë³´ ìˆ˜ì§‘
                episode_infos = self.training_env.get_attr('_episode_count')
                success_counts = self.training_env.get_attr('_success_count')
                
                for i, (episode_count, success_count) in enumerate(zip(episode_infos, success_counts)):
                    # ê¸°ë³¸ ì—í”¼ì†Œë“œ ì •ë³´
                    episode_data = {
                        'timestep': self.num_timesteps,
                        'env_id': i,
                        'episode_count': episode_count,
                        'success_count': success_count,
                        'success_rate': success_count / max(1, episode_count),
                        'timestamp': time.time()
                    }
                    
                    # âœ¨ [ê°œì„ ] í™˜ê²½ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œë„
                    try:
                        # ê°œë³„ í™˜ê²½ ì¸ìŠ¤í„´ìŠ¤ì— ì ‘ê·¼
                        env_instance = self.training_env.envs[i]
                        
                        # ìƒì„¸ ì—í”¼ì†Œë“œ ì •ë³´ ìˆ˜ì§‘
                        if hasattr(env_instance, 'get_detailed_episode_info'):
                            detailed_info = env_instance.get_detailed_episode_info()
                            episode_data['detailed_info'] = detailed_info
                        
                        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                        if hasattr(env_instance, 'get_performance_metrics'):
                            performance_metrics = env_instance.get_performance_metrics()
                            episode_data['performance_metrics'] = performance_metrics
                        
                    except Exception as e:
                        # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©
                        episode_data['detailed_info_error'] = str(e)
                    
                    self.episode_data.append(episode_data)
            except Exception as e:
                # ì „ì²´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
                basic_data = {
                    'timestep': self.num_timesteps,
                    'env_id': 0,
                    'episode_count': 0,
                    'success_count': 0,
                    'success_rate': 0.0,
                    'timestamp': time.time(),
                    'collection_error': str(e)
                }
                self.episode_data.append(basic_data)
    
    def _collect_environment_state(self):
        """í™˜ê²½ ìƒíƒœ ì •ë³´ ìˆ˜ì§‘ - ê°œì„ ëœ ë²„ì „"""
        if hasattr(self.training_env, 'get_attr'):
            try:
                # í™˜ê²½ì˜ ì£¼ìš” ìƒíƒœ ì •ë³´ ìˆ˜ì§‘
                env_states = {}
                
                # ì»¤ë¦¬í˜ëŸ¼ ì •ë³´
                if hasattr(self.training_env, 'get_attr'):
                    try:
                        rand_powers = self.training_env.get_attr('_rand_power')
                        env_states['rand_power'] = rand_powers
                    except:
                        pass
                
                # âœ¨ [ê°œì„ ] í™˜ê²½ì˜ ìƒì„¸ ì„¤ì • ì •ë³´ ìˆ˜ì§‘ ì‹œë„
                try:
                    # ì²« ë²ˆì§¸ í™˜ê²½ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ í™˜ê²½ ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
                    first_env = self.training_env.envs[0]
                    if hasattr(first_env, 'get_environment_summary'):
                        env_summary = first_env.get_environment_summary()
                        env_states['environment_summary'] = env_summary
                except Exception as e:
                    env_states['environment_summary_error'] = str(e)
                
                # í™˜ê²½ ì„¤ì • ì •ë³´
                env_states.update({
                    'timestep': self.num_timesteps,
                    'timestamp': time.time(),
                    'num_envs': self.training_env.num_envs,
                    'frame_skip': getattr(self.training_env, 'frame_skip', None),
                })
                
                self.environment_states.append(env_states)
            except Exception as e:
                # ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
                basic_env_state = {
                    'timestep': self.num_timesteps,
                    'timestamp': time.time(),
                    'num_envs': getattr(self.training_env, 'num_envs', 1),
                    'collection_error': str(e)
                }
                self.environment_states.append(basic_env_state)
    
    def _collect_learning_metrics(self):
        """í•™ìŠµ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ - ë³´ìƒ, ì†ì‹¤, ì„±ê³µë¥  ë“±"""
        try:
            # âœ¨ [ìˆ˜ì •] modelì´ ì¡´ì¬í•˜ëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
            if not hasattr(self, 'model') or self.model is None:
                return
            
            # í˜„ì¬ í•™ìŠµ ìƒíƒœ ì •ë³´
            learning_metrics = {
                'timestep': self.num_timesteps,
                'timestamp': time.time(),
                'total_timesteps': getattr(self.model, 'num_timesteps', 0),
                'learning_starts': getattr(self.model, 'learning_starts', 0),
                'train_freq': getattr(self.model, 'train_freq', None),
            }
            
            # ëª¨ë¸ ìƒíƒœ ì •ë³´
            if hasattr(self.model, 'policy'):
                policy = self.model.policy
                learning_metrics.update({
                    'policy_type': type(policy).__name__,
                    'policy_device': str(getattr(policy, 'device', 'unknown')),
                    'policy_learning_rate': getattr(policy, 'learning_rate', None),
                })
            
            # ì˜µí‹°ë§ˆì´ì € ì •ë³´
            if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'optimizer'):
                optimizer = self.model.policy.optimizer
                learning_metrics.update({
                    'optimizer_type': type(optimizer).__name__,
                    'optimizer_lr': optimizer.param_groups[0]['lr'] if optimizer.param_groups else None,
                })
            
            # í•™ìŠµ ì§„í–‰ë¥  (ì „ì²´ íƒ€ì„ìŠ¤í… ëŒ€ë¹„)
            if hasattr(self.model, 'num_timesteps'):
                total_planned = getattr(self, 'total_timesteps', 0)
                if total_planned > 0:
                    learning_metrics['training_progress'] = min(1.0, self.model.num_timesteps / total_planned)
            
            # ì—í”¼ì†Œë“œ ì„±ê³¼ ìš”ì•½
            if self.episode_data:
                recent_episodes = self.episode_data[-10:]  # ìµœê·¼ 10ê°œ ì—í”¼ì†Œë“œ
                if recent_episodes:
                    success_rates = [ep.get('success_rate', 0) for ep in recent_episodes]
                    learning_metrics.update({
                        'recent_success_rate_avg': float(np.mean(success_rates)),
                        'recent_success_rate_std': float(np.std(success_rates)),
                        'recent_episodes_count': len(recent_episodes),
                    })
            
            # í™˜ê²½ ì„±ëŠ¥ ìš”ì•½
            if self.environment_states:
                recent_env_states = self.environment_states[-5:]  # ìµœê·¼ 5ê°œ í™˜ê²½ ìƒíƒœ
                if recent_env_states:
                    # ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰ë¥ 
                    rand_powers = [state.get('rand_power', [0]) for state in recent_env_states if 'rand_power' in state]
                    if rand_powers and rand_powers[0]:
                        avg_rand_power = float(np.mean([np.mean(powers) for powers in rand_powers if powers]))
                        learning_metrics['curriculum_rand_power_avg'] = avg_rand_power
                        learning_metrics['curriculum_progress'] = max(0, 1.0 - avg_rand_power)  # rand_powerê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§„í–‰ë¨
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                learning_metrics['memory_usage_mb'] = float(memory_info.rss / 1024 / 1024)
            except:
                pass
            
            self.learning_metrics.append(learning_metrics)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            if len(self.learning_metrics) > 100:
                self.learning_metrics = self.learning_metrics[-100:]
                
        except Exception as e:
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
            basic_metrics = {
                'timestep': self.num_timesteps,
                'timestamp': time.time(),
                'collection_error': str(e)
            }
            self.learning_metrics.append(basic_metrics)
    
    def _save_realtime_data(self):
        """ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥
        if self.episode_data:
            episode_file = f"{self.save_dir}/episodes/episodes_{timestamp}.json"
            try:
                with open(episode_file, 'w', encoding='utf-8') as f:
                    json.dump(self.episode_data, f, indent=2, ensure_ascii=False)
                if self.verbose > 0:
                    print(f"ğŸ’¾ ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥: {episode_file}")
            except Exception as e:
                print(f"âŒ ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            if len(self.episode_data) > 100:
                self.episode_data = self.episode_data[-100:]
        
        # í™˜ê²½ ìƒíƒœ ì €ì¥
        if self.environment_states:
            env_file = f"{self.save_dir}/environment_states/env_states_{timestamp}.json"
            try:
                with open(env_file, 'w', encoding='utf-8') as f:
                    json.dump(self.environment_states, f, indent=2, ensure_ascii=False)
                if self.verbose > 0:
                    print(f"ğŸ’¾ í™˜ê²½ ìƒíƒœ ì €ì¥: {env_file}")
            except Exception as e:
                print(f"âŒ í™˜ê²½ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ìµœê·¼ 50ê°œë§Œ ìœ ì§€)
            if len(self.environment_states) > 50:
                self.environment_states = self.environment_states[-50:]
        
        # âœ¨ [ì‹ ê·œ ì¶”ê°€] í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥
        if self.learning_metrics:
            metrics_file = f"{self.save_dir}/learning_metrics/learning_metrics_{timestamp}.json"
            try:
                # ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(os.path.dirname(metrics_file), exist_ok=True)
                
                with open(metrics_file, 'w', encoding='utf-8') as f:
                    json.dump(self.learning_metrics, f, indent=2, ensure_ascii=False)
                if self.verbose > 0:
                    print(f"ğŸ’¾ í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_file}")
            except Exception as e:
                print(f"âŒ í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ìµœê·¼ 50ê°œë§Œ ìœ ì§€)
            if len(self.learning_metrics) > 50:
                self.learning_metrics = self.learning_metrics[-50:]
    
    def _save_hyperparameters(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° í™˜ê²½ ì„¤ì • ì €ì¥"""
        # âœ¨ [ìˆ˜ì •] modelì´ ì¡´ì¬í•˜ëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
        if not hasattr(self, 'model') or self.model is None:
            return
            
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        hyper_file = f"{self.save_dir}/hyperparameters/hyperparameters_{timestamp}.json"
        
        try:
            # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
            hyperparameters = {
                'model_type': 'PPO',
                'policy_type': 'MlpPolicy',
                'learning_rate': getattr(self.model, 'learning_rate', None),
                'n_steps': getattr(self.model, 'n_steps', None),
                'batch_size': getattr(self.model, 'batch_size', None),
                'n_epochs': getattr(self.model, 'n_epochs', None),
                'gamma': getattr(self.model, 'gamma', None),
                'gae_lambda': getattr(self.model, 'gae_lambda', None),
                'clip_range': getattr(self.model, 'clip_range', None),
                'clip_range_vf': getattr(self.model, 'clip_range_vf', None),
                'ent_coef': getattr(self.model, 'ent_coef', None),
                'vf_coef': getattr(self.model, 'vf_coef', None),
                'max_grad_norm': getattr(self.model, 'max_grad_norm', None),
                'use_sde': getattr(self.model, 'use_sde', None),
                'sde_sample_freq': getattr(self.model, 'sde_sample_freq', None),
                'target_kl': getattr(self.model, 'target_kl', None),
                'tensorboard_log': getattr(self.model, 'tensorboard_log', None),
                'policy_kwargs': getattr(self.model, 'policy_kwargs', None),
                'verbose': getattr(self.model, 'verbose', None),
                'seed': getattr(self.model, 'seed', None),
                'device': str(getattr(self.model, 'device', None)),
                'timestamp': timestamp,
                'training_start_time': time.time()
            }
            
            # í™˜ê²½ ì„¤ì • ì •ë³´
            if hasattr(self.training_env, 'get_attr'):
                try:
                    env_configs = []
                    for i in range(self.training_env.num_envs):
                        env_config = {
                            'env_id': i,
                            'ctrl_type': getattr(self.training_env.envs[i], 'ctrl_type', None),
                            'biped': getattr(self.training_env.envs[i], 'biped', None),
                            'rand_power': getattr(self.training_env.envs[i], '_rand_power', None),
                            'action_noise': getattr(self.training_env.envs[i], '_action_noise_scale', None),
                            'frame_skip': getattr(self.training_env.envs[i], 'frame_skip', None),
                            'max_episode_time_sec': getattr(self.training_env.envs[i], '_max_episode_time_sec', None),
                        }
                        env_configs.append(env_config)
                    hyperparameters['environment_configs'] = env_configs
                except:
                    pass
            
            with open(hyper_file, 'w', encoding='utf-8') as f:
                json.dump(hyperparameters, f, indent=2, ensure_ascii=False)
            
            if self.verbose > 0:
                print(f"ğŸ’¾ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥: {hyper_file}")
                
        except Exception as e:
            print(f"âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_checkpoint(self):
        """í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ - ì‹¤ì œ ëª¨ë¸ íŒŒì¼ê³¼ ë©”íƒ€ë°ì´í„° ëª¨ë‘ ì €ì¥"""
        # âœ¨ [ìˆ˜ì •] modelì´ ì¡´ì¬í•˜ëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
        if not hasattr(self, 'model') or self.model is None:
            return
            
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        checkpoint_dir = f"{self.save_dir}/checkpoints/checkpoint_{timestamp}"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # 1. JSON ë©”íƒ€ë°ì´í„° ì €ì¥
        checkpoint_meta_file = f"{checkpoint_dir}/checkpoint_meta.json"
        
        try:
            checkpoint_data = {
                'timestep': self.num_timesteps,
                'timestamp': timestamp,
                'model_info': {
                    'model_type': type(self.model).__name__,
                    'policy_type': getattr(self.model, 'policy', None),
                    'learning_rate': getattr(self.model, 'learning_rate', None),
                },
                'training_info': {
                    'total_timesteps': getattr(self.model, 'num_timesteps', None),
                    'learning_starts': getattr(self.model, 'learning_starts', None),
                    'train_freq': getattr(self.model, 'train_freq', None),
                },
                'environment_info': {
                    'num_envs': self.training_env.num_envs,
                    'observation_space': str(self.training_env.observation_space),
                    'action_space': str(self.training_env.action_space),
                },
                'recovery_info': {
                    'checkpoint_time': time.time(),
                    'can_resume': True,
                    'resume_instructions': "ì´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì¬ê°œí•˜ë ¤ë©´ train.pyì—ì„œ --model_path ì¸ìë¡œ í•´ë‹¹ ëª¨ë¸ íŒŒì¼ì„ ì§€ì •í•˜ì„¸ìš”.",
                    'model_file_path': f"{checkpoint_dir}/model.zip"
                }
            }
            
            with open(checkpoint_meta_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
            
            # 2. ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ì €ì¥ (.zip)
            model_file_path = f"{checkpoint_dir}/model.zip"
            try:
                self.model.save(model_file_path)
                if self.verbose > 0:
                    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ì €ì¥: {model_file_path}")
            except Exception as model_save_error:
                print(f"âŒ ëª¨ë¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {model_save_error}")
                checkpoint_data['model_save_error'] = str(model_save_error)
                # ë©”íƒ€ë°ì´í„°ì— ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
                with open(checkpoint_meta_file, 'w', encoding='utf-8') as f:
                    json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
            
            if self.verbose > 0:
                print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥: {checkpoint_meta_file}")
                print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {checkpoint_dir}")
                
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def on_training_end(self) -> None:
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ë°ì´í„° ì €ì¥"""
        if self.verbose > 0:
            print("ğŸ”š í•™ìŠµ ì¢…ë£Œ - ìµœì¢… ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # ìµœì¢… ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥
        self._save_realtime_data()
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if self.save_checkpoints:
            self._save_checkpoint()
        
        # í•™ìŠµ ì™„ë£Œ ìš”ì•½ ì €ì¥
        self._save_training_summary()
        
        if self.verbose > 0:
            print("âœ… ì‹¤ì‹œê°„ ì €ì¥ ì™„ë£Œ!")
    
    def _save_training_summary(self):
        """í•™ìŠµ ì™„ë£Œ ìš”ì•½ ì €ì¥"""
        # âœ¨ [ìˆ˜ì •] modelì´ ì¡´ì¬í•˜ëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
        if not hasattr(self, 'model') or self.model is None:
            return
            
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        summary_file = f"{self.save_dir}/training_summary_{timestamp}.json"
        
        try:
            summary = {
                'training_completed': True,
                'final_timestep': self.num_timesteps,
                'total_episodes': sum([data.get('episode_count', 0) for data in self.episode_data]),
                'total_successes': sum([data.get('success_count', 0) for data in self.episode_data]),
                'final_success_rate': self.episode_data[-1].get('success_rate', 0) if self.episode_data else 0,
                'training_duration': time.time() - self.hyperparameters_saved if self.hyperparameters_saved else 0,
                'completion_timestamp': timestamp,
                'data_files': {
                    'episodes_dir': f"{self.save_dir}/episodes",
                    'environment_states_dir': f"{self.save_dir}/environment_states",
                    'checkpoints_dir': f"{self.save_dir}/checkpoints",
                    'hyperparameters_dir': f"{self.save_dir}/hyperparameters"
                }
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            if self.verbose > 0:
                print(f"ğŸ’¾ í•™ìŠµ ìš”ì•½ ì €ì¥: {summary_file}")
                
        except Exception as e:
            print(f"âŒ í•™ìŠµ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")


class ComprehensiveSavingCallback(BaseCallback):
    """
    ì¢…í•©ì ì¸ ì‹¤ì‹œê°„ ì €ì¥ì„ ìœ„í•œ ë©”ì¸ ì½œë°± - ëª¨ë“  ì €ì¥ ê¸°ëŠ¥ì„ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.
    ê¸°ì¡´ ì½œë°±ë“¤ê³¼ ì™„ë²½íˆ í˜¸í™˜ë˜ë©°, ê¸°ì¡´ ê¸°ëŠ¥ì€ ì „í˜€ ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        save_dir: str = "comprehensive_data",
        save_frequency: int = 1000,
        checkpoint_frequency: int = 10000,
        verbose: int = 1,
        **kwargs
    ):
        super().__init__(verbose)
        self.save_dir = save_dir
        self.save_frequency = save_frequency
        self.checkpoint_frequency = checkpoint_frequency
        
        # í•˜ìœ„ ì½œë°±ë“¤ ì´ˆê¸°í™”
        self.realtime_saver = RealTimeSavingCallback(
            save_dir=f"{save_dir}/realtime",
            save_frequency=save_frequency,
            checkpoint_frequency=checkpoint_frequency,
            verbose=verbose,
            **kwargs
        )
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(f"{self.save_dir}/realtime", exist_ok=True)
        os.makedirs(f"{self.save_dir}/logs", exist_ok=True)
        
        if verbose > 0:
            print(f"ğŸ”§ ì¢…í•© ì €ì¥ ì½œë°± ì´ˆê¸°í™” ì™„ë£Œ: {self.save_dir}")
            print(f"   - ì‹¤ì‹œê°„ ì €ì¥: {save_frequency} ìŠ¤í…ë§ˆë‹¤")
            print(f"   - ì²´í¬í¬ì¸íŠ¸: {checkpoint_frequency} ìŠ¤í…ë§ˆë‹¤")
    
    def _on_training_start(self) -> None:
        """í•™ìŠµ ì‹œì‘ ì‹œ í•˜ìœ„ ì½œë°±ë“¤ ì´ˆê¸°í™”"""
        self.realtime_saver._on_training_start()
    
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í•˜ìœ„ ì½œë°±ë“¤ ì‹¤í–‰"""
        return self.realtime_saver._on_step()
    
    def on_training_end(self) -> None:
        """í•™ìŠµ ì¢…ë£Œ ì‹œ í•˜ìœ„ ì½œë°±ë“¤ ì •ë¦¬"""
        self.realtime_saver.on_training_end()
        
        # ì¢…í•© ìš”ì•½ ìƒì„±
        self._create_comprehensive_summary()
    
    def _create_comprehensive_summary(self):
        """ì¢…í•©ì ì¸ í•™ìŠµ ìš”ì•½ ìƒì„±"""
        # âœ¨ [ìˆ˜ì •] modelì´ ì¡´ì¬í•˜ëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
        if not hasattr(self, 'model') or self.model is None:
            return
            
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        summary_file = f"{self.save_dir}/comprehensive_summary_{timestamp}.json"
        
        try:
            # ì‹¤ì‹œê°„ ë°ì´í„° ìš”ì•½
            realtime_summary = {
                'realtime_data_dir': f"{self.save_dir}/realtime",
                'total_files': len(os.listdir(f"{self.save_dir}/realtime")),
                'episodes_data': len(os.listdir(f"{self.save_dir}/realtime/episodes")),
                'environment_states': len(os.listdir(f"{self.save_dir}/realtime/environment_states")),
                'checkpoints': len(os.listdir(f"{self.save_dir}/realtime/checkpoints")),
                'hyperparameters': len(os.listdir(f"{self.save_dir}/realtime/hyperparameters")),
            }
            
            # ì „ì²´ ìš”ì•½
            comprehensive_summary = {
                'training_session': {
                    'start_time': timestamp,
                    'completion_time': timestamp,
                    'status': 'completed',
                    'total_timesteps': getattr(self.model, 'num_timesteps', 0),
                },
                'data_storage': realtime_summary,
                'compatibility': {
                    'with_existing_callbacks': True,
                    'with_existing_save_systems': True,
                    'data_format': 'JSON',
                    'encoding': 'UTF-8',
                },
                'recovery_instructions': {
                    'resume_training': "train.py --model_path [ëª¨ë¸ê²½ë¡œ] ì‚¬ìš©",
                    'load_data': "realtime_data ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ í™•ì¸",
                    'checkpoints': "checkpoints ë””ë ‰í† ë¦¬ì—ì„œ ë³µêµ¬ ì§€ì  í™•ì¸",
                }
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_summary, f, indent=2, ensure_ascii=False)
            
            if self.verbose > 0:
                print(f"ğŸ’¾ ì¢…í•© ìš”ì•½ ì €ì¥: {summary_file}")
                print("ğŸ‰ ëª¨ë“  ì‹¤ì‹œê°„ ì €ì¥ ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                
        except Exception as e:
            print(f"âŒ ì¢…í•© ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")